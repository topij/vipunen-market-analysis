{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fc4faad",
   "metadata": {},
   "source": [
    "!/usr/bin/env python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959f364",
   "metadata": {},
   "source": [
    "# Vipunen Education Market Analysis Notebook\n",
    "\n",
    "This script replicates the analysis workflow from the Vipunen project,\n",
    "displaying intermediate results and plots in a way suitable for notebooks\n",
    "or interactive environments, before exporting the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2288a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import sys # Keep sys for potential path manipulation if needed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23b7b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path (for local environment)\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf31e45",
   "metadata": {},
   "source": [
    "#### Project Module Imports\n",
    "Assuming the script is run from the project root or the environment includes the src path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03adb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.vipunen.config.config_loader import get_config\n",
    "    from src.vipunen.data.data_loader import load_data\n",
    "    from src.vipunen.data.data_processor import clean_and_prepare_data\n",
    "    from src.vipunen.export.excel_exporter import export_to_excel\n",
    "    from src.vipunen.analysis.market_analyzer import MarketAnalyzer\n",
    "    # Import the wrapper function from analyze_cli\n",
    "    from src.vipunen.cli.analyze_cli import export_analysis_results\n",
    "    # Import Visualizer and constants if needed later\n",
    "    from src.vipunen.visualization.education_visualizer import EducationVisualizer, COLOR_PALETTES, TEXT_CONSTANTS\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Ensure the script is run from the project root or 'src' is in the Python path.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717906a9",
   "metadata": {},
   "source": [
    "### Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7f29e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, # Use INFO level for notebook clarity, DEBUG can be verbose\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    # Force logging to stdout for notebook-like output\n",
    "    stream=sys.stdout \n",
    ")\n",
    "\n",
    "#Silence overly verbose loggers\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"FileUtils\").setLevel(logging.WARNING) \n",
    "\n",
    "logger = logging.getLogger(\"AnalysisNotebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492d016",
   "metadata": {},
   "source": [
    "## Analysis Configuration\n",
    "Replaces Command-Line Args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e03a09",
   "metadata": {},
   "source": [
    "### Load main project config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03f5fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:10:30,648 - AnalysisNotebook - INFO - --- Setting Analysis Configuration ---\n",
      "2025-05-03 14:10:30,649 - AnalysisNotebook - INFO - Successfully loaded project configuration.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Setting Analysis Configuration ---\")\n",
    "try:\n",
    "    config = get_config() # Assumes config.yaml is discoverable\n",
    "    logger.info(\"Successfully loaded project configuration.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load project configuration: {e}. Exiting.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a3886",
   "metadata": {},
   "source": [
    "Define analysis parameters (equivalent to args parsed in analyze_cli.py)\n",
    "These can be modified by the user for different analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78b49fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:22:33,482 - AnalysisNotebook - INFO - Using Analysis Parameters: {'data_file': 'amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv', 'institution': None, 'short_name': None, 'use_dummy': False, 'filter_qual_types': False, 'output_dir': None, 'include_timestamp': True}\n"
     ]
    }
   ],
   "source": [
    "ANALYSIS_PARAMS = {\n",
    "    'data_file': \"amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\", # Set to None to use path from config, or provide a specific path string\n",
    "    'institution': None, # Set to None to use default from config, or provide an institution key (e.g., 'careeria')\n",
    "    'short_name': None, # Set to None to use default from config based on institution\n",
    "    'use_dummy': False, # Set to True to use dummy data if available\n",
    "    'filter_qual_types': False, # Set to True to filter for specific qualification types (e.g., Ammatti/Erikoisammattitutkinto)\n",
    "    'output_dir': None, # Set to None to use path from config, or provide specific base dir for outputs\n",
    "    'include_timestamp': True # Whether to include timestamp in output filenames\n",
    "    # Add other parameters as needed, mirroring analyze_cli args\n",
    "}\n",
    "\n",
    "logger.info(f\"Using Analysis Parameters: {ANALYSIS_PARAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf9dc3",
   "metadata": {},
   "source": [
    "## Main Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3927ff2a",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Data (Load, Clean, Filter) ==="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8bfe",
   "metadata": {},
   "source": [
    "Setup & Parameter Resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0caa2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:30:54,780 - AnalysisNotebook - INFO - --- Starting Step 1: Preparing Analysis Data ---\n",
      "2025-05-03 14:30:54,781 - AnalysisNotebook - INFO - Resolving parameters and determining institution...\n",
      "2025-05-03 14:30:54,782 - AnalysisNotebook - INFO - Analyzing Institution: Rastor-instituutti (Key: default, Short: RI)\n",
      "2025-05-03 14:30:54,782 - AnalysisNotebook - INFO - Using Institution Variants: ['Rastor-instituutti ry', 'Rastor-instituutti', 'RASTOR OY', 'Rastor Oy']\n",
      "2025-05-03 14:30:54,783 - AnalysisNotebook - INFO - Data Source: amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\n",
      "2025-05-03 14:30:54,787 - AnalysisNotebook - INFO - Initial setup parameters resolved.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Starting Step 1: Preparing Analysis Data ---\")\n",
    "logger.info(\"Resolving parameters and determining institution...\")\n",
    "\n",
    "try:\n",
    "    # Resolve parameters from ANALYSIS_PARAMS and config\n",
    "    data_file_path = ANALYSIS_PARAMS.get('data_file') or config['paths']['data']\n",
    "    use_dummy = ANALYSIS_PARAMS.get('use_dummy', False)\n",
    "    filter_qual_types_flag = ANALYSIS_PARAMS.get('filter_qual_types', False)\n",
    "\n",
    "    # Determine institution key and variants\n",
    "    default_institution_name = config['institutions']['default']['name']\n",
    "    arg_institution_value = ANALYSIS_PARAMS.get('institution')\n",
    "\n",
    "    if arg_institution_value is None or arg_institution_value == default_institution_name:\n",
    "        institution_key = 'default'\n",
    "    else:\n",
    "        institution_key = arg_institution_value\n",
    "        if institution_key not in config['institutions']:\n",
    "            # Use logger.error and raise instead of sys.exit\n",
    "            err_msg = f\"Institution key '{institution_key}' from ANALYSIS_PARAMS not found in config.\"\n",
    "            logger.error(err_msg)\n",
    "            raise KeyError(err_msg)\n",
    "\n",
    "    institution_config = config['institutions'][institution_key]\n",
    "    institution_name = institution_config['name']\n",
    "    institution_short_name = ANALYSIS_PARAMS.get('short_name') or institution_config['short_name']\n",
    "\n",
    "    # Get variants (handle potential absence in config gracefully)\n",
    "    institution_variants = list(institution_config.get('variants', []))\n",
    "    if institution_name not in institution_variants:\n",
    "        institution_variants.append(institution_name)\n",
    "\n",
    "    logger.info(f\"Analyzing Institution: {institution_name} (Key: {institution_key}, Short: {institution_short_name})\")\n",
    "    logger.info(f\"Using Institution Variants: {institution_variants}\")\n",
    "    logger.info(f\"Data Source: {data_file_path}\")\n",
    "\n",
    "    # Define variables needed later but initialized/checked here or in subsequent cells\n",
    "    df_raw = None\n",
    "    df_prepared = None\n",
    "    data_update_date_str = 'N/A'\n",
    "    input_cols = config.get('columns', {}).get('input', {}) # Get input cols config here\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Configuration Error during initial setup: Missing key {e}\")\n",
    "    raise # Re-raise the exception to stop notebook execution\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during initial setup: {e}\", exc_info=True)\n",
    "    raise # Re-raise the exception\n",
    "\n",
    "logger.info(\"Initial setup parameters resolved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d821da6",
   "metadata": {},
   "source": [
    "#### 1a - Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51d31927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:31:30,241 - AnalysisNotebook - INFO - --- Step 1a: Loading Raw Data ---\n",
      "2025-05-03 14:31:30,244 - src.vipunen.data.data_loader - INFO - Attempting to load data from amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\n",
      "2025-05-03 14:31:30,245 - src.vipunen.data.data_loader - WARNING - Input type 'raw' not found in path 'amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv'. Assuming full path is filename relative to 'raw'.\n",
      "2025-05-03 14:31:30,245 - src.vipunen.data.data_loader - INFO - Calling FileUtils.load_single_file('amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv', input_type='raw')\n",
      "2025-05-03 14:31:30,361 - AnalysisNotebook - INFO - Loaded 60838 rows of raw data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw Data Sample (First 5 Rows) ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tilastovuosi</th>\n",
       "      <th>suorituksenTyyppi</th>\n",
       "      <th>tutkintotyyppi</th>\n",
       "      <th>tutkinto</th>\n",
       "      <th>koulutuksenJarjestaja</th>\n",
       "      <th>hankintakoulutuksenJarjestaja</th>\n",
       "      <th>hankintakoulutusKyllaEi</th>\n",
       "      <th>koodiTutkinto</th>\n",
       "      <th>koodiKoulutuksenJarjestaja</th>\n",
       "      <th>koodiHankintakoulutuksenJarjestaja</th>\n",
       "      <th>uudetOpiskelijatLkm</th>\n",
       "      <th>opiskelijatLkm</th>\n",
       "      <th>tutkinnonSuorittaneetLkm</th>\n",
       "      <th>nettoopiskelijamaaraLkm</th>\n",
       "      <th>tietojoukkoPaivitettyPvm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018</td>\n",
       "      <td>Tutkinnon osa/osia</td>\n",
       "      <td>Muu ammatillinen koulutus</td>\n",
       "      <td>Tieto puuttuu</td>\n",
       "      <td>ABB Oy</td>\n",
       "      <td>Tieto puuttuu</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>0763403-0</td>\n",
       "      <td>-1</td>\n",
       "      <td>78</td>\n",
       "      <td>93</td>\n",
       "      <td>50</td>\n",
       "      <td>44.506849</td>\n",
       "      <td>2025-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018</td>\n",
       "      <td>Koko tutkinto</td>\n",
       "      <td>Ammattitutkinnot</td>\n",
       "      <td>Ajoneuvoalan ammattitutkinto</td>\n",
       "      <td>AEL-Amiedu Oy</td>\n",
       "      <td>Tieto puuttuu</td>\n",
       "      <td>False</td>\n",
       "      <td>354345</td>\n",
       "      <td>3008326-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.964384</td>\n",
       "      <td>2025-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>Koko tutkinto</td>\n",
       "      <td>Ammattitutkinnot</td>\n",
       "      <td>Asioimistulkkauksen ammattitutkinto</td>\n",
       "      <td>AEL-Amiedu Oy</td>\n",
       "      <td>Tieto puuttuu</td>\n",
       "      <td>False</td>\n",
       "      <td>384201</td>\n",
       "      <td>3008326-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3.509589</td>\n",
       "      <td>2025-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018</td>\n",
       "      <td>Koko tutkinto</td>\n",
       "      <td>Ammattitutkinnot</td>\n",
       "      <td>Auto- ja kuljetusalan työnjohdon ammattitutkinto</td>\n",
       "      <td>AEL-Amiedu Oy</td>\n",
       "      <td>Tieto puuttuu</td>\n",
       "      <td>False</td>\n",
       "      <td>354315</td>\n",
       "      <td>3008326-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>14.706849</td>\n",
       "      <td>2025-04-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>Koko tutkinto</td>\n",
       "      <td>Erikoisammattitutkinnot</td>\n",
       "      <td>Autoalan myyjän erikoisammattitutkinto</td>\n",
       "      <td>AEL-Amiedu Oy</td>\n",
       "      <td>Tieto puuttuu</td>\n",
       "      <td>False</td>\n",
       "      <td>437108</td>\n",
       "      <td>3008326-5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>2025-04-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tilastovuosi   suorituksenTyyppi             tutkintotyyppi  \\\n",
       "0          2018  Tutkinnon osa/osia  Muu ammatillinen koulutus   \n",
       "1          2018       Koko tutkinto           Ammattitutkinnot   \n",
       "2          2018       Koko tutkinto           Ammattitutkinnot   \n",
       "3          2018       Koko tutkinto           Ammattitutkinnot   \n",
       "4          2018       Koko tutkinto    Erikoisammattitutkinnot   \n",
       "\n",
       "                                           tutkinto koulutuksenJarjestaja  \\\n",
       "0                                     Tieto puuttuu                ABB Oy   \n",
       "1                      Ajoneuvoalan ammattitutkinto         AEL-Amiedu Oy   \n",
       "2               Asioimistulkkauksen ammattitutkinto         AEL-Amiedu Oy   \n",
       "3  Auto- ja kuljetusalan työnjohdon ammattitutkinto         AEL-Amiedu Oy   \n",
       "4            Autoalan myyjän erikoisammattitutkinto         AEL-Amiedu Oy   \n",
       "\n",
       "  hankintakoulutuksenJarjestaja  hankintakoulutusKyllaEi  koodiTutkinto  \\\n",
       "0                 Tieto puuttuu                    False             -1   \n",
       "1                 Tieto puuttuu                    False         354345   \n",
       "2                 Tieto puuttuu                    False         384201   \n",
       "3                 Tieto puuttuu                    False         354315   \n",
       "4                 Tieto puuttuu                    False         437108   \n",
       "\n",
       "  koodiKoulutuksenJarjestaja koodiHankintakoulutuksenJarjestaja  \\\n",
       "0                  0763403-0                                 -1   \n",
       "1                  3008326-5                                 -1   \n",
       "2                  3008326-5                                 -1   \n",
       "3                  3008326-5                                 -1   \n",
       "4                  3008326-5                                 -1   \n",
       "\n",
       "   uudetOpiskelijatLkm  opiskelijatLkm  tutkinnonSuorittaneetLkm  \\\n",
       "0                   78              93                        50   \n",
       "1                    3               3                         0   \n",
       "2                    6               8                         1   \n",
       "3                   14              18                         0   \n",
       "4                    1               2                         0   \n",
       "\n",
       "   nettoopiskelijamaaraLkm tietojoukkoPaivitettyPvm  \n",
       "0                44.506849               2025-04-03  \n",
       "1                 0.964384               2025-04-03  \n",
       "2                 3.509589               2025-04-03  \n",
       "3                14.706849               2025-04-03  \n",
       "4                 0.027397               2025-04-03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:31:30,378 - AnalysisNotebook - INFO - Raw data loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60838 entries, 0 to 60837\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   tilastovuosi                        60838 non-null  int64  \n",
      " 1   suorituksenTyyppi                   60838 non-null  object \n",
      " 2   tutkintotyyppi                      60838 non-null  object \n",
      " 3   tutkinto                            60838 non-null  object \n",
      " 4   koulutuksenJarjestaja               60838 non-null  object \n",
      " 5   hankintakoulutuksenJarjestaja       60838 non-null  object \n",
      " 6   hankintakoulutusKyllaEi             60838 non-null  bool   \n",
      " 7   koodiTutkinto                       60838 non-null  int64  \n",
      " 8   koodiKoulutuksenJarjestaja          60838 non-null  object \n",
      " 9   koodiHankintakoulutuksenJarjestaja  60838 non-null  object \n",
      " 10  uudetOpiskelijatLkm                 60838 non-null  int64  \n",
      " 11  opiskelijatLkm                      60838 non-null  int64  \n",
      " 12  tutkinnonSuorittaneetLkm            60838 non-null  int64  \n",
      " 13  nettoopiskelijamaaraLkm             60838 non-null  float64\n",
      " 14  tietojoukkoPaivitettyPvm            60838 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(5), object(8)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Step 1a: Loading Raw Data ---\")\n",
    "\n",
    "try:\n",
    "    # Check if data_file_path is valid before proceeding\n",
    "    # if not data_file_path or not Path(data_file_path).exists():\n",
    "    #      err_msg = f\"Data file path is invalid or file does not exist: {data_file_path}\"\n",
    "    #      logger.error(err_msg)\n",
    "    #      raise FileNotFoundError(err_msg)\n",
    "\n",
    "    df_raw = load_data(file_path=data_file_path, use_dummy=use_dummy)\n",
    "    logger.info(f\"Loaded {len(df_raw)} rows of raw data.\")\n",
    "\n",
    "    # Display sample and info in notebook\n",
    "    print(\"\\n--- Raw Data Sample (First 5 Rows) ---\")\n",
    "    from IPython.display import display\n",
    "    display(df_raw.head())\n",
    "    print(\"\\n--- Raw Data Info ---\")\n",
    "    # Use buffer to capture info output for cleaner display if needed, or print directly\n",
    "    # import io\n",
    "    # buffer = io.StringIO()\n",
    "    # df_raw.info(buf=buffer, verbose=True, show_counts=True)\n",
    "    # print(buffer.getvalue())\n",
    "    df_raw.info(verbose=True, show_counts=True) # Direct print often works fine\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Data File Error: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during raw data loading: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"Raw data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9dfe63",
   "metadata": {},
   "source": [
    "#### 1b - Extract Data Update Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77ee6e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:33:52,124 - AnalysisNotebook - INFO - --- Step 1b: Extracting Data Update Date ---\n",
      "2025-05-03 14:33:52,126 - AnalysisNotebook - INFO - Extracted data update date: 03.04.2025\n",
      "2025-05-03 14:33:52,127 - AnalysisNotebook - INFO - Date extraction step complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Update Date: 03.04.2025\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Step 1b: Extracting Data Update Date ---\")\n",
    "\n",
    "# Default value if extraction fails\n",
    "data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\")\n",
    "\n",
    "try:\n",
    "    # Ensure df_raw exists and is not empty from the previous step\n",
    "    if df_raw is None or df_raw.empty:\n",
    "        logger.warning(\"Raw data (df_raw) is empty or not loaded. Cannot extract update date. Using current date.\")\n",
    "    else:\n",
    "        update_date_col = input_cols.get('update_date', 'tietojoukkoPaivitettyPvm') # Use input_cols from Cell 1\n",
    "        if update_date_col in df_raw.columns:\n",
    "            try:\n",
    "                # Attempt to parse the date from the first row\n",
    "                raw_date_str = str(df_raw[update_date_col].iloc[0])\n",
    "                # Handle potential NaT or empty strings before parsing\n",
    "                if pd.notna(raw_date_str) and raw_date_str.strip():\n",
    "                    parsed_date = pd.to_datetime(raw_date_str)\n",
    "                    data_update_date_str = parsed_date.strftime(\"%d.%m.%Y\")\n",
    "                    logger.info(f\"Extracted data update date: {data_update_date_str}\")\n",
    "                else:\n",
    "                    logger.warning(f\"Update date value in column '{update_date_col}' is missing or empty. Using current date.\")\n",
    "            except Exception as date_err:\n",
    "                logger.warning(f\"Could not parse date from column '{update_date_col}' value '{raw_date_str}': {date_err}. Using current date.\")\n",
    "        else:\n",
    "            logger.warning(f\"Update date column '{update_date_col}' not found in raw data. Using current date.\")\n",
    "\n",
    "except Exception as e:\n",
    "     logger.error(f\"An unexpected error occurred during date extraction: {e}\", exc_info=True)\n",
    "     # Decide if this error should stop execution or just use the default date\n",
    "     # raise # Uncomment to stop execution on error\n",
    "\n",
    "print(f\"Data Update Date: {data_update_date_str}\")\n",
    "logger.info(\"Date extraction step complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "963626a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:16:04,740 - AnalysisNotebook - INFO - --- Step 1c: Cleaning and Preparing Data (Initial) ---\n",
      "2025-05-03 14:16:04,747 - AnalysisNotebook - ERROR - Raw data (df_raw) is not available. Cannot perform initial cleaning.\n",
      "2025-05-03 14:16:04,748 - AnalysisNotebook - ERROR - An unexpected error occurred during initial data cleaning: df_raw is not defined or loaded.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/68/_1c8pqwx5rvc9hm34fnkft1m0000gp/T/ipykernel_26366/29072811.py\", line 9, in <module>\n",
      "    raise ValueError(\"df_raw is not defined or loaded.\")\n",
      "ValueError: df_raw is not defined or loaded.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "df_raw is not defined or loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m      logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw data (df_raw) is not available. Cannot perform initial cleaning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_raw is not defined or loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying initial cleaning (merging qualifications, shortening names)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m df_clean_initial \u001b[38;5;241m=\u001b[39m clean_and_prepare_data(\n\u001b[1;32m     13\u001b[0m     df_raw,\n\u001b[1;32m     14\u001b[0m     institution_names\u001b[38;5;241m=\u001b[39minstitution_variants, \u001b[38;5;66;03m# From Cell 1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     merge_qualifications\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m     shorten_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: df_raw is not defined or loaded."
     ]
    }
   ],
   "source": [
    "# Cell 4: 1c - Clean and Prepare Data (Initial)\n",
    "logger.info(\"--- Step 1c: Cleaning and Preparing Data (Initial) ---\")\n",
    "df_clean_initial = None # Initialize\n",
    "\n",
    "try:\n",
    "    # Ensure df_raw is available\n",
    "    if df_raw is None:\n",
    "         logger.error(\"Raw data (df_raw) is not available. Cannot perform initial cleaning.\")\n",
    "         raise ValueError(\"df_raw is not defined or loaded.\")\n",
    "\n",
    "    logger.info(\"Applying initial cleaning (merging qualifications, shortening names)...\")\n",
    "    df_clean_initial = clean_and_prepare_data(\n",
    "        df_raw,\n",
    "        institution_names=institution_variants, # From Cell 1\n",
    "        merge_qualifications=True,\n",
    "        shorten_names=True\n",
    "    )\n",
    "    logger.info(f\"Initial cleaning complete. Shape: {df_clean_initial.shape}\")\n",
    "\n",
    "    # Display sample and info\n",
    "    print(\"\\n--- Cleaned Data Sample (Initial) ---\")\n",
    "    from IPython.display import display\n",
    "    display(df_clean_initial.head())\n",
    "    print(\"\\n--- Cleaned Data Info (Initial) ---\")\n",
    "    df_clean_initial.info(verbose=True, show_counts=True)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during initial data cleaning: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"Initial data cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "13f5b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:16:07,771 - AnalysisNotebook - INFO - --- Step 1d: Filtering by Institution's Offered Qualifications ---\n",
      "2025-05-03 14:16:07,773 - AnalysisNotebook - ERROR - Cleaned initial data (df_clean_initial) is not available. Cannot filter.\n",
      "2025-05-03 14:16:07,773 - AnalysisNotebook - ERROR - Data Error during filtering: df_clean_initial is not defined or loaded.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "df_clean_initial is not defined or loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df_clean_initial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m      logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCleaned initial data (df_clean_initial) is not available. Cannot filter.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m      \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_clean_initial is not defined or loaded.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering data based on qualifications offered by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstitution_short_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Ensure required columns exist before filtering\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: df_clean_initial is not defined or loaded."
     ]
    }
   ],
   "source": [
    "# Cell 5: 1d - Filter by Institution's Offered Qualifications\n",
    "logger.info(\"--- Step 1d: Filtering by Institution's Offered Qualifications ---\")\n",
    "df_prepared = None # Initialize\n",
    "\n",
    "try:\n",
    "    # Ensure df_clean_initial is available\n",
    "    if df_clean_initial is None:\n",
    "         logger.error(\"Cleaned initial data (df_clean_initial) is not available. Cannot filter.\")\n",
    "         raise ValueError(\"df_clean_initial is not defined or loaded.\")\n",
    "\n",
    "    logger.info(f\"Filtering data based on qualifications offered by {institution_short_name}...\")\n",
    "\n",
    "    # Ensure required columns exist before filtering\n",
    "    required_filter_cols = [input_cols['provider'], input_cols['subcontractor'], input_cols['qualification']]\n",
    "    if not all(col in df_clean_initial.columns for col in required_filter_cols):\n",
    "        err_msg = f\"Missing one or more required columns for filtering: {required_filter_cols}. Available: {df_clean_initial.columns.tolist()}\"\n",
    "        logger.error(err_msg)\n",
    "        raise ValueError(err_msg)\n",
    "\n",
    "    # Find qualifications offered by the institution (provider or subcontractor)\n",
    "    institution_mask = (\n",
    "        (df_clean_initial[input_cols['provider']].isin(institution_variants)) |\n",
    "        (df_clean_initial[input_cols['subcontractor']].isin(institution_variants))\n",
    "    )\n",
    "    inst_qualifications = df_clean_initial.loc[institution_mask, input_cols['qualification']].unique()\n",
    "\n",
    "    if len(inst_qualifications) > 0:\n",
    "        # Filter the *entire* dataset to include only rows matching these qualifications\n",
    "        df_prepared = df_clean_initial[df_clean_initial[input_cols['qualification']].isin(inst_qualifications)].copy()\n",
    "        logger.info(f\"Filtered data to {len(inst_qualifications)} qualifications associated with {institution_short_name}. Final shape: {df_prepared.shape}\")\n",
    "    else:\n",
    "        logger.warning(f\"No specific qualifications found associated with {institution_short_name} (variants: {institution_variants}). Using data before this filtering step.\")\n",
    "        df_prepared = df_clean_initial.copy() # Use the data before this specific filtering step\n",
    "\n",
    "    # Display final prepared data sample and info\n",
    "    print(\"\\n--- Prepared Data Sample (Final for Analysis) ---\")\n",
    "    from IPython.display import display\n",
    "    display(df_prepared.head())\n",
    "    print(\"\\n--- Prepared Data Info (Final for Analysis) ---\")\n",
    "    df_prepared.info(verbose=True, show_counts=True)\n",
    "\n",
    "except ValueError as e:\n",
    "     logger.error(f\"Data Error during filtering: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during qualification filtering: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"--- Step 1: Data Preparation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba503bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:11:28,520 - AnalysisNotebook - INFO - --- Starting Analysis Workflow ---\n",
      "2025-05-03 14:11:28,522 - AnalysisNotebook - INFO - --- Step 1: Preparing Analysis Data ---\n",
      "2025-05-03 14:11:28,523 - AnalysisNotebook - INFO - Analyzing Institution: Rastor-instituutti (Key: default, Short: RI)\n",
      "2025-05-03 14:11:28,524 - AnalysisNotebook - INFO - Using Institution Variants: ['Rastor-instituutti ry', 'Rastor-instituutti', 'RASTOR OY', 'Rastor Oy']\n",
      "2025-05-03 14:11:28,524 - AnalysisNotebook - INFO - Data Source: data/raw/amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\n",
      "2025-05-03 14:11:28,524 - AnalysisNotebook - INFO - Loading raw data...\n",
      "2025-05-03 14:11:28,525 - src.vipunen.data.data_loader - INFO - Attempting to load data from data/raw/amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\n",
      "2025-05-03 14:11:28,525 - src.vipunen.data.data_loader - INFO - Calling FileUtils.load_single_file('amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv', input_type='raw')\n",
      "2025-05-03 14:11:28,622 - AnalysisNotebook - INFO - Loaded 60838 rows of raw data.\n",
      "2025-05-03 14:11:28,636 - AnalysisNotebook - INFO - Extracted data update date: 03.04.2025\n",
      "2025-05-03 14:11:28,636 - AnalysisNotebook - INFO - Cleaning and preparing data (merging qualifications, shortening names)...\n",
      "2025-05-03 14:11:28,645 - src.vipunen.data.data_processor - INFO - Replaced 'Tieto puuttuu' with NaN in hankintakoulutuksenJarjestaja column\n",
      "2025-05-03 14:11:28,659 - src.vipunen.data.data_processor - INFO - Merged Yrittäjän ammattitutkinto into Yrittäjyyden ammattitutkinto:\n",
      "2025-05-03 14:11:28,660 - src.vipunen.data.data_processor - INFO -   - Before: 594 rows with old name, 380 rows with new name\n",
      "2025-05-03 14:11:28,660 - src.vipunen.data.data_processor - INFO -   - After: 0 rows with old name, 974 rows with new name\n",
      "2025-05-03 14:11:28,705 - src.vipunen.data.data_processor - INFO - Shortened qualification names (erikoisammattitutkinto → EAT, ammattitutkinto → AT)\n",
      "2025-05-03 14:11:28,710 - AnalysisNotebook - INFO - Initial cleaning complete. Shape: (60838, 15)\n",
      "2025-05-03 14:11:28,721 - AnalysisNotebook - INFO - Filtering data based on qualifications offered by RI...\n",
      "2025-05-03 14:11:28,729 - AnalysisNotebook - INFO - Filtered data to 37 qualifications offered by RI. Final shape: (13456, 15)\n",
      "2025-05-03 14:11:28,734 - AnalysisNotebook - INFO - --- Step 1: Data Preparation Complete ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Raw Data Sample (First 5 Rows) ---\n",
      "   tilastovuosi   suorituksenTyyppi             tutkintotyyppi  \\\n",
      "0          2018  Tutkinnon osa/osia  Muu ammatillinen koulutus   \n",
      "1          2018       Koko tutkinto           Ammattitutkinnot   \n",
      "2          2018       Koko tutkinto           Ammattitutkinnot   \n",
      "3          2018       Koko tutkinto           Ammattitutkinnot   \n",
      "4          2018       Koko tutkinto    Erikoisammattitutkinnot   \n",
      "\n",
      "                                           tutkinto koulutuksenJarjestaja  \\\n",
      "0                                     Tieto puuttuu                ABB Oy   \n",
      "1                      Ajoneuvoalan ammattitutkinto         AEL-Amiedu Oy   \n",
      "2               Asioimistulkkauksen ammattitutkinto         AEL-Amiedu Oy   \n",
      "3  Auto- ja kuljetusalan työnjohdon ammattitutkinto         AEL-Amiedu Oy   \n",
      "4            Autoalan myyjän erikoisammattitutkinto         AEL-Amiedu Oy   \n",
      "\n",
      "  hankintakoulutuksenJarjestaja  hankintakoulutusKyllaEi  koodiTutkinto  \\\n",
      "0                 Tieto puuttuu                    False             -1   \n",
      "1                 Tieto puuttuu                    False         354345   \n",
      "2                 Tieto puuttuu                    False         384201   \n",
      "3                 Tieto puuttuu                    False         354315   \n",
      "4                 Tieto puuttuu                    False         437108   \n",
      "\n",
      "  koodiKoulutuksenJarjestaja koodiHankintakoulutuksenJarjestaja  \\\n",
      "0                  0763403-0                                 -1   \n",
      "1                  3008326-5                                 -1   \n",
      "2                  3008326-5                                 -1   \n",
      "3                  3008326-5                                 -1   \n",
      "4                  3008326-5                                 -1   \n",
      "\n",
      "   uudetOpiskelijatLkm  opiskelijatLkm  tutkinnonSuorittaneetLkm  \\\n",
      "0                   78              93                        50   \n",
      "1                    3               3                         0   \n",
      "2                    6               8                         1   \n",
      "3                   14              18                         0   \n",
      "4                    1               2                         0   \n",
      "\n",
      "   nettoopiskelijamaaraLkm tietojoukkoPaivitettyPvm  \n",
      "0                44.506849               2025-04-03  \n",
      "1                 0.964384               2025-04-03  \n",
      "2                 3.509589               2025-04-03  \n",
      "3                14.706849               2025-04-03  \n",
      "4                 0.027397               2025-04-03  \n",
      "\n",
      "--- Raw Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60838 entries, 0 to 60837\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   tilastovuosi                        60838 non-null  int64  \n",
      " 1   suorituksenTyyppi                   60838 non-null  object \n",
      " 2   tutkintotyyppi                      60838 non-null  object \n",
      " 3   tutkinto                            60838 non-null  object \n",
      " 4   koulutuksenJarjestaja               60838 non-null  object \n",
      " 5   hankintakoulutuksenJarjestaja       60838 non-null  object \n",
      " 6   hankintakoulutusKyllaEi             60838 non-null  bool   \n",
      " 7   koodiTutkinto                       60838 non-null  int64  \n",
      " 8   koodiKoulutuksenJarjestaja          60838 non-null  object \n",
      " 9   koodiHankintakoulutuksenJarjestaja  60838 non-null  object \n",
      " 10  uudetOpiskelijatLkm                 60838 non-null  int64  \n",
      " 11  opiskelijatLkm                      60838 non-null  int64  \n",
      " 12  tutkinnonSuorittaneetLkm            60838 non-null  int64  \n",
      " 13  nettoopiskelijamaaraLkm             60838 non-null  float64\n",
      " 14  tietojoukkoPaivitettyPvm            60838 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(5), object(8)\n",
      "memory usage: 6.6+ MB\n",
      "\n",
      "--- Cleaned Data Sample (Initial) ---\n",
      "   tilastovuosi   suorituksenTyyppi             tutkintotyyppi  \\\n",
      "0          2018  Tutkinnon osa/osia  Muu ammatillinen koulutus   \n",
      "1          2018       Koko tutkinto           Ammattitutkinnot   \n",
      "2          2018       Koko tutkinto           Ammattitutkinnot   \n",
      "3          2018       Koko tutkinto           Ammattitutkinnot   \n",
      "4          2018       Koko tutkinto    Erikoisammattitutkinnot   \n",
      "\n",
      "                              tutkinto koulutuksenJarjestaja  \\\n",
      "0                        Tieto puuttuu                ABB Oy   \n",
      "1                      Ajoneuvoalan AT         AEL-Amiedu Oy   \n",
      "2               Asioimistulkkauksen AT         AEL-Amiedu Oy   \n",
      "3  Auto- ja kuljetusalan työnjohdon AT         AEL-Amiedu Oy   \n",
      "4                  Autoalan myyjän EAT         AEL-Amiedu Oy   \n",
      "\n",
      "  hankintakoulutuksenJarjestaja  hankintakoulutusKyllaEi  koodiTutkinto  \\\n",
      "0                           NaN                    False             -1   \n",
      "1                           NaN                    False         354345   \n",
      "2                           NaN                    False         384201   \n",
      "3                           NaN                    False         354315   \n",
      "4                           NaN                    False         437108   \n",
      "\n",
      "  koodiKoulutuksenJarjestaja koodiHankintakoulutuksenJarjestaja  \\\n",
      "0                  0763403-0                                 -1   \n",
      "1                  3008326-5                                 -1   \n",
      "2                  3008326-5                                 -1   \n",
      "3                  3008326-5                                 -1   \n",
      "4                  3008326-5                                 -1   \n",
      "\n",
      "   uudetOpiskelijatLkm  opiskelijatLkm  tutkinnonSuorittaneetLkm  \\\n",
      "0                   78              93                        50   \n",
      "1                    3               3                         0   \n",
      "2                    6               8                         1   \n",
      "3                   14              18                         0   \n",
      "4                    1               2                         0   \n",
      "\n",
      "   nettoopiskelijamaaraLkm tietojoukkoPaivitettyPvm  \n",
      "0                44.506849               2025-04-03  \n",
      "1                 0.964384               2025-04-03  \n",
      "2                 3.509589               2025-04-03  \n",
      "3                14.706849               2025-04-03  \n",
      "4                 0.027397               2025-04-03  \n",
      "\n",
      "--- Cleaned Data Info (Initial) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60838 entries, 0 to 60837\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   tilastovuosi                        60838 non-null  int64  \n",
      " 1   suorituksenTyyppi                   60838 non-null  object \n",
      " 2   tutkintotyyppi                      60838 non-null  object \n",
      " 3   tutkinto                            60838 non-null  object \n",
      " 4   koulutuksenJarjestaja               60838 non-null  object \n",
      " 5   hankintakoulutuksenJarjestaja       14804 non-null  object \n",
      " 6   hankintakoulutusKyllaEi             60838 non-null  bool   \n",
      " 7   koodiTutkinto                       60838 non-null  int64  \n",
      " 8   koodiKoulutuksenJarjestaja          60838 non-null  object \n",
      " 9   koodiHankintakoulutuksenJarjestaja  60838 non-null  object \n",
      " 10  uudetOpiskelijatLkm                 60838 non-null  int64  \n",
      " 11  opiskelijatLkm                      60838 non-null  int64  \n",
      " 12  tutkinnonSuorittaneetLkm            60838 non-null  int64  \n",
      " 13  nettoopiskelijamaaraLkm             60838 non-null  float64\n",
      " 14  tietojoukkoPaivitettyPvm            60838 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(5), object(8)\n",
      "memory usage: 6.6+ MB\n",
      "\n",
      "--- Prepared Data Sample (Final for Analysis) ---\n",
      "    tilastovuosi suorituksenTyyppi           tutkintotyyppi  \\\n",
      "1           2018     Koko tutkinto         Ammattitutkinnot   \n",
      "5           2018     Koko tutkinto  Erikoisammattitutkinnot   \n",
      "15          2018     Koko tutkinto         Ammattitutkinnot   \n",
      "16          2018     Koko tutkinto  Erikoisammattitutkinnot   \n",
      "17          2018     Koko tutkinto  Erikoisammattitutkinnot   \n",
      "\n",
      "                              tutkinto koulutuksenJarjestaja  \\\n",
      "1                      Ajoneuvoalan AT         AEL-Amiedu Oy   \n",
      "5              Autoalan työnjohdon EAT         AEL-Amiedu Oy   \n",
      "15                      Isännöinnin AT         AEL-Amiedu Oy   \n",
      "16                      Johtamisen EAT         AEL-Amiedu Oy   \n",
      "17  Johtamisen ja yritysjohtamisen EAT         AEL-Amiedu Oy   \n",
      "\n",
      "   hankintakoulutuksenJarjestaja  hankintakoulutusKyllaEi  koodiTutkinto  \\\n",
      "1                            NaN                    False         354345   \n",
      "5                            NaN                    False         457305   \n",
      "15                           NaN                    False         334103   \n",
      "16                           NaN                    False         437101   \n",
      "17                           NaN                    False         437141   \n",
      "\n",
      "   koodiKoulutuksenJarjestaja koodiHankintakoulutuksenJarjestaja  \\\n",
      "1                   3008326-5                                 -1   \n",
      "5                   3008326-5                                 -1   \n",
      "15                  3008326-5                                 -1   \n",
      "16                  3008326-5                                 -1   \n",
      "17                  3008326-5                                 -1   \n",
      "\n",
      "    uudetOpiskelijatLkm  opiskelijatLkm  tutkinnonSuorittaneetLkm  \\\n",
      "1                     3               3                         0   \n",
      "5                     1               5                         0   \n",
      "15                   28              28                         0   \n",
      "16                  132             189                         8   \n",
      "17                   14              14                         0   \n",
      "\n",
      "    nettoopiskelijamaaraLkm tietojoukkoPaivitettyPvm  \n",
      "1                  0.964384               2025-04-03  \n",
      "5                  4.997260               2025-04-03  \n",
      "15                13.139726               2025-04-03  \n",
      "16               115.156164               2025-04-03  \n",
      "17                 1.112329               2025-04-03  \n",
      "\n",
      "--- Prepared Data Info (Final for Analysis) ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 13456 entries, 1 to 60836\n",
      "Data columns (total 15 columns):\n",
      " #   Column                              Non-Null Count  Dtype  \n",
      "---  ------                              --------------  -----  \n",
      " 0   tilastovuosi                        13456 non-null  int64  \n",
      " 1   suorituksenTyyppi                   13456 non-null  object \n",
      " 2   tutkintotyyppi                      13456 non-null  object \n",
      " 3   tutkinto                            13456 non-null  object \n",
      " 4   koulutuksenJarjestaja               13456 non-null  object \n",
      " 5   hankintakoulutuksenJarjestaja       5731 non-null   object \n",
      " 6   hankintakoulutusKyllaEi             13456 non-null  bool   \n",
      " 7   koodiTutkinto                       13456 non-null  int64  \n",
      " 8   koodiKoulutuksenJarjestaja          13456 non-null  object \n",
      " 9   koodiHankintakoulutuksenJarjestaja  13456 non-null  object \n",
      " 10  uudetOpiskelijatLkm                 13456 non-null  int64  \n",
      " 11  opiskelijatLkm                      13456 non-null  int64  \n",
      " 12  tutkinnonSuorittaneetLkm            13456 non-null  int64  \n",
      " 13  nettoopiskelijamaaraLkm             13456 non-null  float64\n",
      " 14  tietojoukkoPaivitettyPvm            13456 non-null  object \n",
      "dtypes: bool(1), float64(1), int64(5), object(8)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# logger.info(\"--- Starting Analysis Workflow ---\")\n",
    "# logger.info(\"--- Step 1: Preparing Analysis Data ---\")\n",
    "\n",
    "# try:\n",
    "#     # Resolve parameters from ANALYSIS_PARAMS and config\n",
    "#     data_file_path = ANALYSIS_PARAMS.get('data_file') or config['paths']['data']\n",
    "#     use_dummy = ANALYSIS_PARAMS.get('use_dummy', False)\n",
    "#     filter_qual_types_flag = ANALYSIS_PARAMS.get('filter_qual_types', False)\n",
    "\n",
    "#     # Determine institution key and variants\n",
    "#     default_institution_name = config['institutions']['default']['name']\n",
    "#     arg_institution_value = ANALYSIS_PARAMS.get('institution')\n",
    "\n",
    "#     if arg_institution_value is None or arg_institution_value == default_institution_name:\n",
    "#         institution_key = 'default'\n",
    "#     else:\n",
    "#         institution_key = arg_institution_value\n",
    "#         if institution_key not in config['institutions']:\n",
    "#             raise KeyError(f\"Institution key '{institution_key}' from ANALYSIS_PARAMS not found in config.\")\n",
    "\n",
    "#     institution_config = config['institutions'][institution_key]\n",
    "#     institution_name = institution_config['name']\n",
    "#     institution_short_name = ANALYSIS_PARAMS.get('short_name') or institution_config['short_name']\n",
    "    \n",
    "#     # Get variants (handle potential absence in config gracefully)\n",
    "#     institution_variants = list(institution_config.get('variants', []))\n",
    "#     if institution_name not in institution_variants:\n",
    "#         institution_variants.append(institution_name)\n",
    "        \n",
    "#     logger.info(f\"Analyzing Institution: {institution_name} (Key: {institution_key}, Short: {institution_short_name})\")\n",
    "#     logger.info(f\"Using Institution Variants: {institution_variants}\")\n",
    "#     logger.info(f\"Data Source: {data_file_path}\")\n",
    "\n",
    "#     # --- 1a: Load Raw Data ---\n",
    "#     logger.info(\"Loading raw data...\")\n",
    "#     df_raw = load_data(file_path=data_file_path, use_dummy=use_dummy)\n",
    "#     logger.info(f\"Loaded {len(df_raw)} rows of raw data.\")\n",
    "#     print(\"\\n--- Raw Data Sample (First 5 Rows) ---\")\n",
    "#     print(df_raw.head())\n",
    "#     print(\"\\n--- Raw Data Info ---\")\n",
    "#     df_raw.info(verbose=True, show_counts=True)\n",
    "\n",
    "#     # --- 1b: Extract Data Update Date ---\n",
    "#     data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\") # Default\n",
    "#     update_date_col = config.get('columns', {}).get('input', {}).get('update_date', 'tietojoukkoPaivitettyPvm')\n",
    "#     if not df_raw.empty and update_date_col in df_raw.columns:\n",
    "#         try:\n",
    "#             raw_date_str = str(df_raw[update_date_col].iloc[0])\n",
    "#             parsed_date = pd.to_datetime(raw_date_str)\n",
    "#             data_update_date_str = parsed_date.strftime(\"%d.%m.%Y\")\n",
    "#             logger.info(f\"Extracted data update date: {data_update_date_str}\")\n",
    "#         except Exception as date_err:\n",
    "#             logger.warning(f\"Could not parse date from column '{update_date_col}': {date_err}. Using current date.\")\n",
    "#     else:\n",
    "#         logger.warning(f\"Update date column '{update_date_col}' not found or data empty. Using current date.\")\n",
    "\n",
    "#     # --- 1c: Clean and Prepare Data (Initial) ---\n",
    "#     logger.info(\"Cleaning and preparing data (merging qualifications, shortening names)...\")\n",
    "#     df_clean_initial = clean_and_prepare_data(\n",
    "#         df_raw,\n",
    "#         institution_names=institution_variants, # Pass variants here for potential use in cleaning\n",
    "#         merge_qualifications=True,\n",
    "#         shorten_names=True\n",
    "#     )\n",
    "#     logger.info(f\"Initial cleaning complete. Shape: {df_clean_initial.shape}\")\n",
    "#     print(\"\\n--- Cleaned Data Sample (Initial) ---\")\n",
    "#     print(df_clean_initial.head())\n",
    "#     print(\"\\n--- Cleaned Data Info (Initial) ---\")\n",
    "#     df_clean_initial.info(verbose=True, show_counts=True)\n",
    "\n",
    "#     # --- 1d: Filter by Institution's Offered Qualifications ---\n",
    "#     logger.info(f\"Filtering data based on qualifications offered by {institution_short_name}...\")\n",
    "#     input_cols = config['columns']['input']\n",
    "#     # Ensure required columns exist before filtering\n",
    "#     required_filter_cols = [input_cols['provider'], input_cols['subcontractor'], input_cols['qualification']]\n",
    "#     if not all(col in df_clean_initial.columns for col in required_filter_cols):\n",
    "#         raise ValueError(f\"Missing one or more required columns for filtering: {required_filter_cols}\")\n",
    "        \n",
    "#     institution_mask = (\n",
    "#         (df_clean_initial[input_cols['provider']].isin(institution_variants)) |\n",
    "#         (df_clean_initial[input_cols['subcontractor']].isin(institution_variants))\n",
    "#     )\n",
    "#     inst_qualifications = df_clean_initial.loc[institution_mask, input_cols['qualification']].unique()\n",
    "\n",
    "#     if len(inst_qualifications) > 0:\n",
    "#         df_prepared = df_clean_initial[df_clean_initial[input_cols['qualification']].isin(inst_qualifications)].copy()\n",
    "#         logger.info(f\"Filtered data to {len(inst_qualifications)} qualifications offered by {institution_short_name}. Final shape: {df_prepared.shape}\")\n",
    "#     else:\n",
    "#         logger.warning(f\"No specific qualifications found for {institution_short_name} based on variants {institution_variants}. Using data before institution qualification filtering.\")\n",
    "#         df_prepared = df_clean_initial # Use the data before this specific filtering step\n",
    "\n",
    "#     print(\"\\n--- Prepared Data Sample (Final for Analysis) ---\")\n",
    "#     print(df_prepared.head())\n",
    "#     print(\"\\n--- Prepared Data Info (Final for Analysis) ---\")\n",
    "#     df_prepared.info(verbose=True, show_counts=True)\n",
    "    \n",
    "#     logger.info(\"--- Step 1: Data Preparation Complete ---\")\n",
    "\n",
    "# except KeyError as e:\n",
    "#     logger.error(f\"Configuration Error during data preparation: Missing key {e}\")\n",
    "#     sys.exit(1)\n",
    "# except FileNotFoundError as e:\n",
    "#     logger.error(f\"Data File Error: {e}\")\n",
    "#     sys.exit(1)\n",
    "# except ValueError as e:\n",
    "#     logger.error(f\"Data Error during preparation: {e}\")\n",
    "#     sys.exit(1)\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"An unexpected error occurred during data preparation: {e}\", exc_info=True)\n",
    "#     sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f13d3",
   "metadata": {},
   "source": [
    "### Step 2: Perform Market Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47bdbdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:04:44,462 - AnalysisNotebook - INFO - --- Step 2: Performing Market Analysis ---\n",
      "2025-05-03 14:04:44,463 - AnalysisNotebook - INFO - Skipping qualification type filtering.\n",
      "2025-05-03 14:04:44,464 - AnalysisNotebook - INFO - Initializing MarketAnalyzer...\n",
      "2025-05-03 14:04:44,465 - AnalysisNotebook - INFO - Analyzer configured for: RI (['Rastor-instituutti ry', 'Rastor-instituutti', 'RASTOR OY', 'Rastor Oy'])\n",
      "2025-05-03 14:04:44,466 - AnalysisNotebook - INFO - Running analysis...\n",
      "2025-05-03 14:04:44,466 - src.vipunen.analysis.market_analyzer - INFO - Calling get_all_results to perform market analysis for: ['Rastor-instituutti ry', 'Rastor-instituutti', 'RASTOR OY', 'Rastor Oy']\n",
      "2025-05-03 14:04:44,466 - src.vipunen.analysis.market_analyzer - INFO - Calculating all analysis results...\n",
      "2025-05-03 14:04:44,467 - src.vipunen.analysis.education_market - INFO - Loaded data with 13456 rows\n",
      "2025-05-03 14:04:44,474 - src.vipunen.analysis.education_market - INFO - DEBUG: Main provider volume for 2018: 2053.0876712333998\n",
      "2025-05-03 14:04:44,478 - src.vipunen.analysis.education_market - INFO - Generated volume summary for 8 years\n",
      "2025-05-03 14:04:44,482 - src.vipunen.analysis.market_analyzer - INFO - Calculated total_volumes. Shape: (8, 6)\n",
      "2025-05-03 14:04:44,619 - src.vipunen.analysis.market_analyzer - INFO - Calculated volumes_by_qualification. Shape: (182, 7)\n",
      "2025-05-03 14:04:44,620 - src.vipunen.analysis.market_analyzer - INFO - Calculating detailed market shares for all providers...\n",
      "2025-05-03 14:04:44,620 - src.vipunen.analysis.market_share_analyzer - WARNING - Using 'both' for share_calculation_basis. Market shares are calculated based on total volume (main provider + subcontractor). This may lead to double counting and the sum of shares per qualification/year exceeding 100%.\n",
      "2025-05-03 14:04:46,483 - src.vipunen.analysis.market_share_analyzer - INFO - Calculated market shares for 6779 provider-qualification combinations\n",
      "2025-05-03 14:04:46,484 - src.vipunen.analysis.market_analyzer - INFO - Calculating market share changes and ranks...\n",
      "2025-05-03 14:04:46,488 - src.vipunen.analysis.market_share_analyzer - INFO - Calculated market share changes for 5272 provider-qualification-year combinations.\n",
      "2025-05-03 14:04:46,490 - AnalysisNotebook - ERROR - An unexpected error occurred during market analysis: 'market_share_growth'\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/68/_1c8pqwx5rvc9hm34fnkft1m0000gp/T/ipykernel_26366/1536847910.py\", line 28, in <module>\n",
      "    analysis_results = analyzer.analyze() # This returns a dict of DataFrames\n",
      "  File \"/Users/topi/data-science/repos/vipunen-project/src/vipunen/analysis/market_analyzer.py\", line 1023, in analyze\n",
      "    return self.get_all_results()\n",
      "  File \"/Users/topi/data-science/repos/vipunen-project/src/vipunen/analysis/market_analyzer.py\", line 972, in get_all_results\n",
      "    results['detailed_providers_market'] = self.calculate_providers_market()\n",
      "  File \"/Users/topi/data-science/repos/vipunen-project/src/vipunen/analysis/market_analyzer.py\", line 297, in calculate_providers_market\n",
      "    detailed_shares = detailed_shares.rename(columns={'market_share_change': self.cols_out['market_share_growth']})\n",
      "KeyError: 'market_share_growth'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 28\u001b[0m\n\u001b[1;32m     27\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# This returns a dict of DataFrames\u001b[39;00m\n\u001b[1;32m     29\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalysis complete. Results keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(analysis_results\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/data-science/repos/vipunen-project/src/vipunen/analysis/market_analyzer.py:1023\u001b[0m, in \u001b[0;36mMarketAnalyzer.analyze\u001b[0;34m(self, min_market_size_threshold)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# The filtering based on threshold is now handled within get_all_results\u001b[39;00m\n\u001b[0;32m-> 1023\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/data-science/repos/vipunen-project/src/vipunen/analysis/market_analyzer.py:972\u001b[0m, in \u001b[0;36mMarketAnalyzer.get_all_results\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated volumes_by_qualification. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolumes_by_qualification\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 972\u001b[0m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetailed_providers_market\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_providers_market\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated detailed_providers_market. Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetailed_providers_market\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/data-science/repos/vipunen-project/src/vipunen/analysis/market_analyzer.py:297\u001b[0m, in \u001b[0;36mMarketAnalyzer.calculate_providers_market\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Rename market_share_change to the config output name\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m detailed_shares \u001b[38;5;241m=\u001b[39m detailed_shares\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarket_share_change\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcols_out\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmarket_share_growth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m})\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Fill NaN growth for the first year or where previous share was zero/NaN\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'market_share_growth'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[25], line 51\u001b[0m\n\u001b[1;32m     50\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# Ensure it's None on error\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Or handle more gracefully depending on desired flow\u001b[39;00m\n",
      "\u001b[0;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[1;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2122\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[1;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[0;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m   1171\u001b[0m ):\n\u001b[1;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[1;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m   1064\u001b[0m )\n\u001b[1;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/vipunen-analytics/lib/python3.9/site-packages/IPython/core/ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[0;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Step 2: Performing Market Analysis ---\")\n",
    "\n",
    "try:\n",
    "    # Ensure data from Step 1 is available\n",
    "    if 'df_prepared' not in locals() or df_prepared is None or df_prepared.empty:\n",
    "        logger.warning(\"Prepared data (df_prepared) is missing or empty. Skipping Market Analysis.\")\n",
    "        analysis_results = {} # Initialize as empty\n",
    "        analyzer = None # Initialize as None\n",
    "    else:\n",
    "        # --- 2a: Apply Optional Qualification Type Filtering ---\n",
    "        if filter_qual_types_flag:\n",
    "            qual_types = config.get('qualification_types', ['Ammattitutkinnot', 'Erikoisammattitutkinnot']) # Default types if not in config\n",
    "            logger.info(f\"Applying qualification type filter for: {qual_types}\")\n",
    "            df_analysis_input = df_prepared[df_prepared['tutkintotyyppi'].isin(qual_types)].copy()\n",
    "            logger.info(f\"Shape before type filter: {df_prepared.shape}, after: {df_analysis_input.shape}\")\n",
    "        else:\n",
    "            logger.info(\"Skipping qualification type filtering.\")\n",
    "            df_analysis_input = df_prepared.copy() # Use the prepared data directly\n",
    "\n",
    "        # --- 2b: Initialize and Run Analyzer ---\n",
    "        logger.info(\"Initializing MarketAnalyzer...\")\n",
    "        analyzer = MarketAnalyzer(df_analysis_input, cfg=config)\n",
    "        analyzer.institution_names = institution_variants\n",
    "        analyzer.institution_short_name = institution_short_name\n",
    "        logger.info(f\"Analyzer configured for: {institution_short_name} ({institution_variants})\")\n",
    "\n",
    "        logger.info(\"Running analysis...\")\n",
    "        analysis_results = analyzer.analyze() # This returns a dict of DataFrames\n",
    "        logger.info(f\"Analysis complete. Results keys: {list(analysis_results.keys())}\")\n",
    "\n",
    "        # --- 2c: Display Sample Results ---\n",
    "        # Print head of key results DataFrames for notebook-like inspection\n",
    "        print(\"\\n--- Analysis Results Samples ---\")\n",
    "        for key, df_result in analysis_results.items():\n",
    "            if isinstance(df_result, pd.DataFrame) and not df_result.empty:\n",
    "                print(f\"\\n--- Result: {key} (Top 5 rows) ---\")\n",
    "                print(df_result.head())\n",
    "            elif isinstance(df_result, pd.DataFrame) and df_result.empty:\n",
    "                 print(f\"\\n--- Result: {key} (DataFrame is empty) ---\")\n",
    "            else:\n",
    "                # Handle non-DataFrame results if any (e.g., scalars, lists)\n",
    "                 print(f\"\\n--- Result: {key} (Type: {type(df_result)}) ---\")\n",
    "                 print(df_result)\n",
    "                 \n",
    "        logger.info(\"--- Step 2: Market Analysis Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during market analysis: {e}\", exc_info=True)\n",
    "    analysis_results = {} # Ensure it exists but is empty on error\n",
    "    analyzer = None # Ensure it's None on error\n",
    "    sys.exit(1) # Or handle more gracefully depending on desired flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3c76c",
   "metadata": {},
   "source": [
    "### Step 3: Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3: Generating Visualizations ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af83c8e",
   "metadata": {},
   "source": [
    "Define directory for saving plots --> Will use main output dir for PDF\n",
    "plots_output_dir = None \n",
    "figures = {} # Initialize dictionary to store figure paths --> Not needed for PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfa58aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:05:03,146 - AnalysisNotebook - WARNING - Analysis results or analyzer instance missing. Skipping Visualizations.\n"
     ]
    }
   ],
   "source": [
    "pdf_report_path = None # Store path to the generated PDF\n",
    "\n",
    "try:\n",
    "    # Ensure results from Step 2 are available\n",
    "    if 'analysis_results' not in locals() or not analysis_results or 'analyzer' not in locals() or analyzer is None:\n",
    "        logger.warning(\"Analysis results or analyzer instance missing. Skipping Visualizations.\")\n",
    "    else:\n",
    "        # Determine base output directory\n",
    "        base_output_path_str = ANALYSIS_PARAMS.get('output_dir') or config.get('paths', {}).get('output', 'output') # Default to 'output'\n",
    "        base_output_path = Path(base_output_path_str)\n",
    "\n",
    "        # Create the specific subdirectory for the institution's report (Excel and PDF)\n",
    "        # This logic might be duplicated in export_analysis_results, but ensures dir exists for Visualizer\n",
    "        report_dir_name = f\"education_market_{institution_short_name.lower()}\"\n",
    "        report_output_dir = base_output_path / report_dir_name\n",
    "        report_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"PDF report will be saved in directory: {report_output_dir}\")\n",
    "\n",
    "        # Initialize Visualizer - For PDF Output\n",
    "        logger.info(\"Initializing EducationVisualizer for PDF output...\")\n",
    "        visualizer = EducationVisualizer(\n",
    "            style=\"default\",\n",
    "            output_dir=report_output_dir, # Use the main report dir\n",
    "            output_format='pdf', # Generate PDF\n",
    "            institution_short_name=institution_short_name,\n",
    "            include_timestamp=ANALYSIS_PARAMS.get('include_timestamp', True) # Match Excel timestamping\n",
    "        )\n",
    "        visualizer.data_update_date = data_update_date_str\n",
    "\n",
    "        logger.info(\"Generating plots and adding them to PDF...\")\n",
    "\n",
    "        # --- Get Config Column Names (using defaults as fallback) ---\n",
    "        cols_out = config.get('columns', {}).get('output', {})\n",
    "        year_col = cols_out.get('year', 'Vuosi') # Assuming Vuosi is common\n",
    "        qual_col = cols_out.get('qualification', 'Tutkinto')\n",
    "        provider_col = cols_out.get('provider', 'Oppilaitos') # Assuming Oppilaitos\n",
    "        provider_amount_col = cols_out.get('provider_amount', 'NOM järjestäjänä')\n",
    "        subcontractor_amount_col = cols_out.get('subcontractor_amount', 'NOM hankintana')\n",
    "        total_volume_col = cols_out.get('total_volume', 'NOM yhteensä')\n",
    "        market_total_col = cols_out.get('market_total', 'Markkina yhteensä')\n",
    "        market_share_col = cols_out.get('market_share', 'Markkinaosuus (%)')\n",
    "        # BCG specific columns (defined in MarketAnalyzer._calculate_bcg_data)\n",
    "        bcg_growth_col = 'Market Growth (%)'\n",
    "        bcg_share_col = 'Relative Market Share'\n",
    "        bcg_size_col = 'Institution Volume'\n",
    "        # Provider count specific columns (defined in MarketAnalyzer._calculate_provider_counts)\n",
    "        count_provider_col = 'Unique_Providers_Count'\n",
    "        count_subcontractor_col = 'Unique_Subcontractors_Count'\n",
    "\n",
    "        # --- Get other needed info ---\n",
    "        inst_short_name = analyzer.institution_short_name\n",
    "        inst_names = analyzer.institution_names\n",
    "        min_year = analyzer.min_year\n",
    "        max_year = analyzer.max_year\n",
    "        base_caption = TEXT_CONSTANTS[\"data_source\"].format(date=data_update_date_str)\n",
    "        last_full_year = max_year - 1 if max_year and min_year and max_year > min_year else max_year\n",
    "        plot_reference_year = last_full_year if last_full_year else max_year\n",
    "\n",
    "        # --- Plot 1: Stacked Area (Total Volumes) ---\n",
    "        total_volumes_df = analysis_results.get('total_volumes')\n",
    "        if total_volumes_df is not None and not total_volumes_df.empty and all(c in total_volumes_df.columns for c in [year_col, provider_amount_col, subcontractor_amount_col]):\n",
    "            try:\n",
    "                logger.info(\"Generating Total Volumes Area Chart...\")\n",
    "                plot_df_roles = total_volumes_df.rename(columns={provider_amount_col: 'järjestäjänä', subcontractor_amount_col: 'hankintana'})\n",
    "                fig, _ = visualizer.create_area_chart(\n",
    "                    data=plot_df_roles, x_col=year_col, y_cols=['järjestäjänä', 'hankintana'],\n",
    "                    colors=[COLOR_PALETTES[\"roles\"][\"järjestäjänä\"], COLOR_PALETTES[\"roles\"][\"hankintana\"]],\n",
    "                    labels=['järjestäjänä', 'hankintana'], title=f\"{inst_short_name} netto-opiskelijamäärä vuosina {min_year}-{max_year}\",\n",
    "                    caption=base_caption, stacked=True\n",
    "                )\n",
    "                # Add figure to PDF\n",
    "                visualizer.save_visualization(fig, f\"{inst_short_name}_total_volumes_area\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Total Volumes plot: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(f\"Skipping Total Volumes plot: Data not available or missing columns.\")\n",
    "\n",
    "        # --- Determine Active Qualifications (logic from analyze_cli.py) ---\n",
    "        detailed_df = analysis_results.get('detailed_providers_market')\n",
    "        active_qualifications = []\n",
    "        if detailed_df is not None and not detailed_df.empty and max_year is not None and min_year is not None:\n",
    "            years_to_check_activity = [last_full_year]\n",
    "            prev_full_year = last_full_year - 1 if last_full_year > min_year else None\n",
    "            if prev_full_year: years_to_check_activity.append(prev_full_year)\n",
    "\n",
    "            inst_recent_df = detailed_df[(detailed_df[provider_col].isin(inst_names)) & (detailed_df[year_col].isin(years_to_check_activity))].copy()\n",
    "            analysis_config = config.get('analysis', {})\n",
    "            min_volume_sum_threshold = analysis_config.get('active_qualification_min_volume_sum', 3)\n",
    "            quals_with_recent_volume = set()\n",
    "            if total_volume_col in inst_recent_df.columns:\n",
    "                volume_grouped = inst_recent_df.groupby(qual_col)[total_volume_col].sum()\n",
    "                quals_with_recent_volume = set(volume_grouped[volume_grouped >= min_volume_sum_threshold].index)\n",
    "            \n",
    "            quals_with_100_share_both_years = set()\n",
    "            if prev_full_year is not None and not inst_recent_df.empty and market_share_col in inst_recent_df.columns:\n",
    "                inst_recent_df[market_share_col] = pd.to_numeric(inst_recent_df[market_share_col], errors='coerce')\n",
    "                inst_100_share_df = inst_recent_df[inst_recent_df[market_share_col].round(2) == 100.0]\n",
    "                share_counts = inst_100_share_df.groupby(qual_col)[year_col].nunique()\n",
    "                quals_with_100_share_both_years = set(share_counts[share_counts == 2].index)\n",
    "                \n",
    "            active_qualifications_set = quals_with_recent_volume - quals_with_100_share_both_years\n",
    "            active_qualifications = sorted(list(active_qualifications_set))\n",
    "            logger.info(f\"Determined {len(active_qualifications)} active qualifications for plots.\")\n",
    "        else:\n",
    "            logger.warning(\"Could not determine active qualifications. Using fallback or skipping related plots.\")\n",
    "            if detailed_df is not None and not detailed_df.empty and provider_col in detailed_df.columns and qual_col in detailed_df.columns:\n",
    "                 active_qualifications = detailed_df[detailed_df[provider_col].isin(inst_names)][qual_col].unique().tolist()\n",
    "                 logger.info(f\"Using fallback: {len(active_qualifications)} qualifications institution ever offered.\")\n",
    "\n",
    "        # --- Plot 2: Line Chart (Market Share Evolution) - Loop per Qualification ---\n",
    "        if detailed_df is not None and not detailed_df.empty:\n",
    "            logger.info(f\"Generating Market Share Line Charts for {len(active_qualifications)} active qualifications...\")\n",
    "            for qual in active_qualifications:\n",
    "                try:\n",
    "                    qual_df = detailed_df[detailed_df[qual_col] == qual]\n",
    "                    if qual_df.empty: continue\n",
    "                    latest_qual_providers = qual_df[qual_df[year_col] == plot_reference_year]\n",
    "                    top_m_providers = latest_qual_providers.nlargest(6, market_share_col)[provider_col].tolist()\n",
    "                    plot_data = qual_df[qual_df[provider_col].isin(top_m_providers)].pivot(index=year_col, columns=provider_col, values=market_share_col)\n",
    "                    if not plot_data.empty:\n",
    "                        plot_data.index.name = year_col # Ensure index name\n",
    "                        qual_filename_part = qual.replace(' ', '_').replace('/', '_').replace(':', '_').replace(',', '').replace('.', '').lower()[:50] # More robust filename part\n",
    "                        fig, _ = visualizer.create_line_chart(\n",
    "                            data=plot_data, x_col=plot_data.index, y_cols=top_m_providers,\n",
    "                            colors=COLOR_PALETTES[\"main\"], labels=top_m_providers,\n",
    "                            title=f\"{qual}: Markkinaosuus (%)\", caption=base_caption, markers=True\n",
    "                        )\n",
    "                        # Add figure to PDF\n",
    "                        visualizer.save_visualization(fig, f\"{inst_short_name}_{qual_filename_part}_market_share_lines\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to generate Market Share line plot for {qual}: {e}\", exc_info=True)\n",
    "                    if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping Market Share Line Charts: Detailed market data missing.\")\n",
    "\n",
    "        # --- Plot 3: Heatmap (Institution's Share) ---\n",
    "        if detailed_df is not None and not detailed_df.empty:\n",
    "            try:\n",
    "                logger.info(\"Generating Institution Market Share Heatmap...\")\n",
    "                inst_share_df_raw = detailed_df[detailed_df[provider_col].isin(inst_names)].copy() # Use .copy()\n",
    "                inst_share_df = inst_share_df_raw # Start with raw, aggregate if multiple variants\n",
    "                \n",
    "                if len(inst_names) > 1 and not inst_share_df_raw.empty:\n",
    "                     # Define aggregation logic using config column names\n",
    "                    agg_logic = {\n",
    "                        provider_amount_col: 'sum', subcontractor_amount_col: 'sum',\n",
    "                        total_volume_col: 'sum', market_total_col: 'first',\n",
    "                        market_share_col: 'first', # Placeholder, recalculate\n",
    "                        # Add other columns if needed, e.g., 'first' for rank\n",
    "                    }\n",
    "                    agg_logic = {k: v for k, v in agg_logic.items() if k in inst_share_df_raw.columns}\n",
    "                    if agg_logic:\n",
    "                        inst_share_df_agg = inst_share_df_raw.groupby([year_col, qual_col], as_index=False).agg(agg_logic)\n",
    "                        # Recalculate market share\n",
    "                        if total_volume_col in inst_share_df_agg.columns and market_total_col in inst_share_df_agg.columns:\n",
    "                            valid_market_total = inst_share_df_agg[market_total_col] > 0\n",
    "                            inst_share_df_agg[market_share_col] = 0.0\n",
    "                            inst_share_df_agg.loc[valid_market_total, market_share_col] = (inst_share_df_agg.loc[valid_market_total, total_volume_col] / inst_share_df_agg.loc[valid_market_total, market_total_col] * 100)\n",
    "                        inst_share_df = inst_share_df_agg\n",
    "                    else:\n",
    "                         logger.warning(\"Aggregation skipped for heatmap: No relevant columns found for aggregation.\")\n",
    "\n",
    "                if not inst_share_df.empty:\n",
    "                    inst_share_df_active = inst_share_df[inst_share_df[qual_col].isin(active_qualifications)]\n",
    "                else: inst_share_df_active = pd.DataFrame() # Ensure it's an empty df\n",
    "                \n",
    "                if not inst_share_df_active.empty and market_share_col in inst_share_df_active.columns:\n",
    "                    heatmap_data = inst_share_df_active.pivot_table(index=qual_col, columns=year_col, values=market_share_col)\n",
    "                    heatmap_data = heatmap_data.sort_index() # Sort rows\n",
    "                    if not heatmap_data.empty:\n",
    "                        fig, _ = visualizer.create_heatmap(\n",
    "                            data=heatmap_data, title=f\"{inst_short_name} markkinaosuus (%) aktiivisissa tutkinnoissa\",\n",
    "                            caption=base_caption, cmap=\"Greens\", annot=True, fmt=\".1f\"\n",
    "                        )\n",
    "                        # Add figure to PDF\n",
    "                        visualizer.save_visualization(fig, f\"{inst_short_name}_market_share_heatmap_active\")\n",
    "                    else: logger.warning(f\"Skipping Heatmap: Pivoted data is empty for active qualifications.\")\n",
    "                else: logger.warning(f\"Skipping Heatmap: No data for {inst_short_name} in active qualifications or missing market share column.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Market Share Heatmap: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping Heatmap: Detailed market data missing.\")\n",
    "\n",
    "        # --- Plot 4: BCG Matrix ---\n",
    "        bcg_data_df = analysis_results.get('bcg_data')\n",
    "        if bcg_data_df is not None and not bcg_data_df.empty:\n",
    "            logger.info(\"Generating BCG Growth-Share Matrix...\")\n",
    "            try:\n",
    "                required_bcg_cols = [qual_col, bcg_growth_col, bcg_share_col, bcg_size_col]\n",
    "                # Check if qual_col from config is present (it might be named differently initially)\n",
    "                actual_qual_col = qual_col if qual_col in bcg_data_df.columns else 'Qualification' # Check common alternative\n",
    "                if actual_qual_col not in bcg_data_df.columns: \n",
    "                    logger.warning(f\"BCG Matrix: Qualification column ('{qual_col}' or 'Qualification') not found.\")\n",
    "                elif all(c in bcg_data_df.columns for c in [bcg_growth_col, bcg_share_col, bcg_size_col]):\n",
    "                    plot_title = f\"{inst_short_name}: Tutkintojen kasvu vs. markkinaosuus ({plot_reference_year})\"\n",
    "                    bcg_caption = base_caption + f\" Kuplan koko = {inst_short_name} volyymi ({plot_reference_year}). Suhteellinen markkinaosuus = {inst_short_name} osuus / Suurimman kilpailijan osuus.\"\n",
    "                    fig, _ = visualizer.create_bcg_matrix(\n",
    "                        data=bcg_data_df, growth_col=bcg_growth_col, share_col=bcg_share_col,\n",
    "                        size_col=bcg_size_col, label_col=actual_qual_col, title=plot_title, caption=bcg_caption\n",
    "                    )\n",
    "                    # Add figure to PDF\n",
    "                    visualizer.save_visualization(fig, f\"{inst_short_name}_bcg_matrix\")\n",
    "                else: logger.warning(f\"Skipping BCG Matrix: Missing required columns. Found: {bcg_data_df.columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate BCG Matrix plot: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping BCG Matrix: bcg_data not available or empty.\")\n",
    "\n",
    "        # --- Plot 5: Combined Volume / Provider Count Plot ---\n",
    "        volume_df = analysis_results.get('total_volumes')\n",
    "        count_df = analysis_results.get('provider_counts_by_year')\n",
    "        if volume_df is not None and not volume_df.empty and count_df is not None and not count_df.empty:\n",
    "            logger.info(\"Generating Volume / Provider Count Plot...\")\n",
    "            try:\n",
    "                # Check if all required columns exist\n",
    "                req_vol_cols = [year_col, provider_amount_col, subcontractor_amount_col]\n",
    "                req_count_cols = [year_col, count_provider_col, count_subcontractor_col]\n",
    "                if all(c in volume_df.columns for c in req_vol_cols) and all(c in count_df.columns for c in req_count_cols):\n",
    "                    plot_title = f\"{inst_short_name}: Opiskelijamäärät ja kouluttajamarkkina ({min_year}-{max_year})\"\n",
    "                    plot_caption = base_caption + f\". Kouluttajamäärä perustuu tutkintoihin, joita {inst_short_name} tarjoaa.\"\n",
    "                    fig, _ = visualizer.create_volume_and_provider_count_plot(\n",
    "                        volume_data=volume_df, count_data=count_df, title=plot_title,\n",
    "                        volume_title=\"Netto-opiskelijamäärä\", count_title=\"Uniikit kouluttajat markkinassa\",\n",
    "                        year_col=year_col, vol_provider_col=provider_amount_col, vol_subcontractor_col=subcontractor_amount_col,\n",
    "                        count_provider_col=count_provider_col, count_subcontractor_col=count_subcontractor_col,\n",
    "                        caption=plot_caption\n",
    "                    )\n",
    "                    # Add figure to PDF\n",
    "                    visualizer.save_visualization(fig, f\"{inst_short_name}_volume_provider_counts\")\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping Volume/Provider Count plot: Missing required columns. Vol needed: {req_vol_cols} (found {volume_df.columns.tolist()}). Count needed: {req_count_cols} (found {count_df.columns.tolist()})\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Volume/Provider Count plot: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping Volume/Provider Count plot: Required volume or count data missing.\")\n",
    "\n",
    "        # --- Finalize and Close PDF ---\n",
    "        logger.info(\"Finalizing PDF report...\")\n",
    "        visualizer.close_pdf() # Close the PDF file\n",
    "        pdf_report_path = visualizer.pdf_path # Get the path where the PDF was saved\n",
    "        if pdf_report_path:\n",
    "             logger.info(f\"PDF report saved to: {pdf_report_path}\")\n",
    "        else:\n",
    "             logger.warning(\"Could not determine PDF report path from visualizer.\")\n",
    "\n",
    "        logger.info(f\"--- Step 3: PDF Report Generation Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during PDF visualization generation: {e}\", exc_info=True)\n",
    "    # figures dict is already initialized -> No figures dict anymore\n",
    "    pdf_report_path = None # Ensure path is None on error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38780b2",
   "metadata": {},
   "source": [
    "=== Step 4: Export Final Results (Excel) ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c46726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:05:08,669 - AnalysisNotebook - INFO - --- Step 4: Exporting Final Results to Excel ---\n",
      "2025-05-03 14:05:08,670 - AnalysisNotebook - WARNING - Analysis results missing. Skipping Excel Export.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Step 4: Exporting Final Results to Excel ---\")\n",
    "\n",
    "excel_path = None # Initialize path\n",
    "try:\n",
    "    # Ensure analysis results are available\n",
    "    if 'analysis_results' not in locals() or not analysis_results:\n",
    "        logger.warning(\"Analysis results missing. Skipping Excel Export.\")\n",
    "    else:\n",
    "        # --- Create Metadata ---\n",
    "        logger.info(\"Creating metadata for export...\")\n",
    "        metadata = {\n",
    "            \"Analysis Timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"Institution Analyzed\": f\"{institution_name} ({institution_short_name})\", # Use variables from Step 1\n",
    "            \"Institution Variants Used\": \", \".join(institution_variants),\n",
    "            \"Input Data File\": data_file_path,\n",
    "            \"Data Update Date\": data_update_date_str,\n",
    "            \"Qualification Type Filter Applied\": \"Yes\" if filter_qual_types_flag else \"No\",\n",
    "            \"Min Market Size Threshold (for plots)\": config.get('analysis', {}).get('min_market_size_threshold', 'N/A'),\n",
    "            \"Active Qualifications Filter Threshold (for plots)\": config.get('analysis', {}).get('active_qualification_min_volume_sum', 'N/A')\n",
    "        }\n",
    "        metadata_df = pd.DataFrame(metadata.items(), columns=[\"Parameter\", \"Value\"])\n",
    "        logger.info(\"Metadata created successfully.\")\n",
    "        print(\"\\n--- Analysis Metadata ---\")\n",
    "        print(metadata_df)\n",
    "        \n",
    "        # --- Determine Base Output Path ---\n",
    "        base_output_path_str = ANALYSIS_PARAMS.get('output_dir') or config.get('paths', {}).get('output', 'output') # Default to 'output'\n",
    "        base_output_path = Path(base_output_path_str)\n",
    "        # No need to create the subdir here, export_to_excel should handle it based on its logic\n",
    "        logger.info(f\"Using base output directory for Excel export: {base_output_path}\")\n",
    "\n",
    "        # --- Call Exporter ---\n",
    "        # Use the wrapper function from analyze_cli which handles filename and dict prep\n",
    "        excel_path = export_analysis_results(\n",
    "            analysis_results=analysis_results,\n",
    "            config=config,\n",
    "            institution_short_name=institution_short_name,\n",
    "            base_output_path=str(base_output_path), # Pass as string\n",
    "            metadata_df=metadata_df,\n",
    "            include_timestamp=ANALYSIS_PARAMS.get('include_timestamp', True)\n",
    "        )\n",
    "        \n",
    "        if excel_path:\n",
    "            logger.info(f\"Successfully exported results to Excel: {excel_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Excel export function did not return a valid path.\")\n",
    "            \n",
    "        logger.info(\"--- Step 4: Excel Export Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during Excel export: {e}\", exc_info=True)\n",
    "    # excel_path remains None or its previous value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9458f0",
   "metadata": {},
   "source": [
    "=== Step 5: Custom User Analysis Area ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c18d844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:05:11,788 - AnalysisNotebook - INFO - --- Step 5: Custom Analysis Area ---\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"--- Step 5: Custom Analysis Area ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b51521",
   "metadata": {},
   "source": [
    "The main analysis workflow is complete.\n",
    "Key variables available for further analysis:\n",
    "- config: The loaded project configuration dictionary.\n",
    "- ANALYSIS_PARAMS: The parameters used for this specific run.\n",
    "- df_raw: The raw data loaded initially.\n",
    "- df_prepared: The cleaned and filtered data used for the main analysis.\n",
    "- analysis_results: A dictionary containing the various analysis DataFrames.\n",
    "    Keys: ['total_volumes', 'volumes_by_qualification', 'detailed_providers_market', \n",
    "           'qualification_cagr', 'overall_total_market_volume', \n",
    "           'qualification_market_yoy_growth', 'provider_counts_by_year', 'bcg_data']\n",
    "- analyzer: The MarketAnalyzer instance.\n",
    "- pdf_report_path: Path to the generated PDF report (if successful).\n",
    "- excel_path: The path to the generated Excel file (if export was successful).\n",
    "- institution_short_name: Short name of the analyzed institution.\n",
    "- institution_variants: List of variants used for matching.\n",
    "- data_update_date_str: The data update date string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9934e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:05:14,952 - AnalysisNotebook - INFO - You can now add your custom Python code below to explore the results further.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example: Accessing the detailed market data DataFrame:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"You can now add your custom Python code below to explore the results further.\")\n",
    "print(\"\\nExample: Accessing the detailed market data DataFrame:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fc414",
   "metadata": {},
   "source": [
    "Uncomment the lines below to print the head of the detailed market data\n",
    "if 'analysis_results' in locals() and 'detailed_providers_market' in analysis_results:\n",
    "    print(\"\\n--- Detailed Providers Market Sample (from analysis_results) ---\")\n",
    "    detailed_df_example = analysis_results['detailed_providers_market']\n",
    "    print(detailed_df_example.head())\n",
    "else:\n",
    "    print(\"Detailed market data not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d63fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nScript execution finished. Add custom analysis code below this line.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434292f",
   "metadata": {},
   "source": [
    "--- End of Script ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c53df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8445fb76",
   "metadata": {},
   "source": [
    "TODO: Add Section for Custom User Analysis --> This is now Step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb63269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 14:05:37,814 - AnalysisNotebook - INFO - Running analysis script...\n",
      "2025-05-03 14:05:37,817 - AnalysisNotebook - INFO - Data preparation step appears complete.\n",
      "2025-05-03 14:05:37,817 - AnalysisNotebook - WARNING - Market analysis step completed but produced no results.\n",
      "2025-05-03 14:05:37,818 - AnalysisNotebook - WARNING - Excel export step may have failed or was skipped.\n",
      "2025-05-03 14:05:37,819 - AnalysisNotebook - INFO - Data path from config: data/raw/amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\n",
      "2025-05-03 14:05:37,819 - AnalysisNotebook - INFO - Analysis script finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block allows running the script directly, mimicking notebook execution\n",
    "    logger.info(\"Running analysis script...\")\n",
    "    \n",
    "    # Placeholder for calling the main workflow steps\n",
    "    if 'df_prepared' in locals():\n",
    "        logger.info(\"Data preparation step appears complete.\")\n",
    "        if 'analysis_results' in locals() and analysis_results:\n",
    "            logger.info(\"Market analysis step appears complete.\")\n",
    "            # Check for PDF path instead of figures dict\n",
    "            if 'pdf_report_path' in locals() and pdf_report_path:\n",
    "                logger.info(\"PDF Visualization step appears complete.\")\n",
    "            else:\n",
    "                logger.warning(\"PDF Visualization step may have failed or was skipped.\")\n",
    "        elif 'analysis_results' in locals():\n",
    "             logger.warning(\"Market analysis step completed but produced no results.\")\n",
    "        else:\n",
    "            logger.warning(\"Market analysis step may have failed, 'analysis_results' not found.\")\n",
    "            \n",
    "        if 'excel_path' in locals() and excel_path:\n",
    "             logger.info(\"Excel export step appears complete.\")\n",
    "        else:\n",
    "             logger.warning(\"Excel export step may have failed or was skipped.\")\n",
    "            \n",
    "    else:\n",
    "        logger.warning(\"Data preparation step may have failed, 'df_prepared' not found.\")\n",
    "\n",
    "    # Example: Accessing a config value\n",
    "    try:\n",
    "        data_path_from_config = config.get('paths', {}).get('data', 'Not Found')\n",
    "        logger.info(f\"Data path from config: {data_path_from_config}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error accessing config: {e}\")\n",
    "        \n",
    "    logger.info(\"Analysis script finished.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be2578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vipunen-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
