{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vipunen Education Market Analysis Notebook\n",
    "\n",
    "This notebook replicates the analysis workflow from the Vipunen project (`run_analysis.py`), providing a step-by-step guide with explanations and intermediate outputs. It aims to:\n",
    "\n",
    "*   Showcase and explain the Vipunen analysis functionality.\n",
    "*   Generate the standard PDF report and Excel export.\n",
    "*   Provide a template for users to perform additional custom analyses using the prepared data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import sys\n",
    "\n",
    "# --- Project Module Imports ---\n",
    "# Assuming the notebook kernel runs from the project root or the environment includes the src path\n",
    "try:\n",
    "    from src.vipunen.config.config_loader import get_config\n",
    "    from src.vipunen.data.data_loader import load_data\n",
    "    from src.vipunen.data.data_processor import clean_and_prepare_data\n",
    "    # excel_exporter not needed directly if using the wrapper\n",
    "    # from src.vipunen.export.excel_exporter import export_to_excel \n",
    "    from src.vipunen.analysis.market_analyzer import MarketAnalyzer\n",
    "    # Import the wrapper function from analyze_cli for Excel export\n",
    "    from src.vipunen.cli.analyze_cli import export_analysis_results \n",
    "    # Import Visualizer and constants\n",
    "    from src.vipunen.visualization.education_visualizer import EducationVisualizer, COLOR_PALETTES, TEXT_CONSTANTS\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Ensure the notebook kernel runs from the project root or 'src' is in the Python path.\")\n",
    "    # Raise error to stop execution if imports fail\n",
    "    raise \n",
    "\n",
    "# --- Logging Configuration ---\n",
    "# Configure logging to output to stdout/stderr, which Jupyter captures\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout, # Explicitly direct to stdout\n",
    "    force=True # Override any existing handlers\n",
    ")\n",
    "# Silence overly verbose loggers\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"FileUtils\").setLevel(logging.WARNING) \n",
    "\n",
    "logger = logging.getLogger(\"AnalysisNotebook\")\n",
    "\n",
    "logger.info(\"Imports and logging configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Configuration\n",
    "\n",
    "Load the main project configuration (`config.yaml`) and define the parameters for this specific analysis run. Modify the `ANALYSIS_PARAMS` dictionary below to change the input data, target institution, or other analysis settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load main project config\n",
    "try:\n",
    "    config = get_config() # Assumes config.yaml is discoverable\n",
    "    logger.info(\"Successfully loaded project configuration.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load project configuration: {e}. Exiting.\")\n",
    "    raise # Stop execution if config fails to load\n",
    "\n",
    "# Define analysis parameters (equivalent to args parsed in analyze_cli.py)\n",
    "# These can be modified by the user for different analyses\n",
    "ANALYSIS_PARAMS = {\n",
    "    'data_file': None, # Set to None to use path from config, or provide a specific path string\n",
    "    'institution': None, # Set to None to use default from config, or provide an institution key (e.g., 'careeria')\n",
    "    'short_name': None, # Set to None to use default from config based on institution\n",
    "    'use_dummy': False, # Set to True to use dummy data if available\n",
    "    'filter_qual_types': False, # Set to True to filter for specific qualification types (e.g., Ammatti/Erikoisammattitutkinto)\n",
    "    'output_dir': None, # Set to None to use path from config, or provide specific base dir for outputs\n",
    "    'include_timestamp': True # Whether to include timestamp in output filenames\n",
    "    # Add other parameters as needed, mirroring analyze_cli args\n",
    "}\n",
    "\n",
    "logger.info(f\"Using Analysis Parameters: {ANALYSIS_PARAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Data (Load, Clean, Filter)\n",
    "\n",
    "This step involves:\n",
    "1.  **Resolving Parameters:** Determining the institution, data path, etc. based on `ANALYSIS_PARAMS` and `config`.\n",
    "2.  **Loading Data:** Reading the raw data file.\n",
    "3.  **Extracting Date:** Getting the data update date from the raw file.\n",
    "4.  **Cleaning:** Performing initial data cleaning (merging qualifications, shortening names).\n",
    "5.  **Filtering:** Filtering the data to include only qualifications relevant to the selected institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the entire data prep step in a try-except block\n",
    "df_raw = None\n",
    "df_prepared = None\n",
    "institution_name = None\n",
    "institution_short_name = None\n",
    "institution_variants = []\n",
    "data_update_date_str = 'N/A'\n",
    "filter_qual_types_flag = False\n",
    "data_file_path = None\n",
    "\n",
    "try:\n",
    "    logger.info(\"--- Step 1: Preparing Analysis Data ---\")\n",
    "    # Resolve parameters from ANALYSIS_PARAMS and config\n",
    "    data_file_path = ANALYSIS_PARAMS.get('data_file') or config['paths']['data']\n",
    "    use_dummy = ANALYSIS_PARAMS.get('use_dummy', False)\n",
    "    filter_qual_types_flag = ANALYSIS_PARAMS.get('filter_qual_types', False)\n",
    "\n",
    "    # Determine institution key and variants\n",
    "    default_institution_name = config['institutions']['default']['name']\n",
    "    arg_institution_value = ANALYSIS_PARAMS.get('institution')\n",
    "\n",
    "    if arg_institution_value is None or arg_institution_value == default_institution_name:\n",
    "        institution_key = 'default'\n",
    "    else:\n",
    "        institution_key = arg_institution_value\n",
    "        if institution_key not in config['institutions']:\n",
    "            raise KeyError(f\"Institution key '{institution_key}' from ANALYSIS_PARAMS not found in config.\")\n",
    "\n",
    "    institution_config = config['institutions'][institution_key]\n",
    "    institution_name = institution_config['name']\n",
    "    institution_short_name = ANALYSIS_PARAMS.get('short_name') or institution_config['short_name']\n",
    "    \n",
    "    # Get variants (handle potential absence in config gracefully)\n",
    "    institution_variants = list(institution_config.get('variants', []))\n",
    "    if institution_name not in institution_variants:\n",
    "        institution_variants.append(institution_name)\n",
    "        \n",
    "    logger.info(f\"Analyzing Institution: {institution_name} (Key: {institution_key}, Short: {institution_short_name})\")\n",
    "    logger.info(f\"Using Institution Variants: {institution_variants}\")\n",
    "    logger.info(f\"Data Source: {data_file_path}\")\n",
    "\n",
    "    # --- 1a: Load Raw Data ---\n",
    "    logger.info(\"Loading raw data...\")\n",
    "    df_raw = load_data(file_path=data_file_path, use_dummy=use_dummy)\n",
    "    logger.info(f\"Loaded {len(df_raw)} rows of raw data.\")\n",
    "    print(\"\n--- Raw Data Sample (First 5 Rows) ---\")\n",
    "    # Display in notebook output\n",
    "    from IPython.display import display \n",
    "    display(df_raw.head()) \n",
    "    print(\"\n--- Raw Data Info ---\")\n",
    "    df_raw.info(verbose=True, show_counts=True)\n",
    "\n",
    "    # --- 1b: Extract Data Update Date ---
",
    "    data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\") # Default\n",
    "    update_date_col = config.get('columns', {}).get('input', {}).get('update_date', 'tietojoukkoPaivitettyPvm')\n",
    "    if not df_raw.empty and update_date_col in df_raw.columns:\n",
    "        try:\n",
    "            raw_date_str = str(df_raw[update_date_col].iloc[0])\n",
    "            parsed_date = pd.to_datetime(raw_date_str)\n",
    "            data_update_date_str = parsed_date.strftime(\"%d.%m.%Y\")\n",
    "            logger.info(f\"Extracted data update date: {data_update_date_str}\")\n",
    "        except Exception as date_err:\n",
    "            logger.warning(f\"Could not parse date from column '{update_date_col}': {date_err}. Using current date.\")\n",
    "    else:\n",
    "        logger.warning(f\"Update date column '{update_date_col}' not found or data empty. Using current date.\")\n",
    "\n",
    "    # --- 1c: Clean and Prepare Data (Initial) ---\n",
    "    logger.info(\"Cleaning and preparing data (merging qualifications, shortening names)...\")\n",
    "    df_clean_initial = clean_and_prepare_data(\n",
    "        df_raw,\n",
    "        institution_names=institution_variants, # Pass variants here for potential use in cleaning\n",
    "        merge_qualifications=True,\n",
    "        shorten_names=True\n",
    "    )\n",
    "    logger.info(f\"Initial cleaning complete. Shape: {df_clean_initial.shape}\")\n",
    "    print(\"\n--- Cleaned Data Sample (Initial) ---\")\n",
    "    display(df_clean_initial.head()) # Display in notebook\n",
    "    print(\"\n--- Cleaned Data Info (Initial) ---\")\n",
    "    df_clean_initial.info(verbose=True, show_counts=True)\n",
    "\n",
    "    # --- 1d: Filter by Institution's Offered Qualifications ---
",
    "    logger.info(f\"Filtering data based on qualifications offered by {institution_short_name}...\")\n",
    "    input_cols = config['columns']['input']\n",
    "    # Ensure required columns exist before filtering\n",
    "    required_filter_cols = [input_cols['provider'], input_cols['subcontractor'], input_cols['qualification']]\n",
    "    if not all(col in df_clean_initial.columns for col in required_filter_cols):\n",
    "        raise ValueError(f\"Missing one or more required columns for filtering: {required_filter_cols}\")\n",
    "        \n",
    "    institution_mask = (\n",
    "        (df_clean_initial[input_cols['provider']].isin(institution_variants)) |\n",
    "        (df_clean_initial[input_cols['subcontractor']].isin(institution_variants))\n",
    "    )\n",
    "    inst_qualifications = df_clean_initial.loc[institution_mask, input_cols['qualification']].unique()\n",
    "\n",
    "    if len(inst_qualifications) > 0:\n",
    "        df_prepared = df_clean_initial[df_clean_initial[input_cols['qualification']].isin(inst_qualifications)].copy()\n",
    "        logger.info(f\"Filtered data to {len(inst_qualifications)} qualifications offered by {institution_short_name}. Final shape: {df_prepared.shape}\")\n",
    "    else:\n",
    "        logger.warning(f\"No specific qualifications found for {institution_short_name} based on variants {institution_variants}. Using data before institution qualification filtering.\")\n",
    "        df_prepared = df_clean_initial # Use the data before this specific filtering step\n",
    "\n",
    "    print(\"\n--- Prepared Data Sample (Final for Analysis) ---\")\n",
    "    display(df_prepared.head()) # Display in notebook\n",
    "    print(\"\n--- Prepared Data Info (Final for Analysis) ---\")\n",
    "    df_prepared.info(verbose=True, show_counts=True)\n",
    "    \n",
    "    logger.info(\"--- Step 1: Data Preparation Complete ---\")\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Configuration Error during data preparation: Missing key {e}\")\n",
    "    # sys.exit(1) -> Remove exit\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Data File Error: {e}\")\n",
    "    # sys.exit(1) -> Remove exit\n",
    "except ValueError as e:\n",
    "    logger.error(f\"Data Error during preparation: {e}\")\n",
    "    # sys.exit(1) -> Remove exit\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during data preparation: {e}\", exc_info=True)\n",
    "    # sys.exit(1) -> Remove exit\n",
    "    raise # Re-raise other exceptions to stop execution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform Market Analysis\n",
    "\n",
    "This step uses the `MarketAnalyzer` class to perform the core analysis on the prepared data.\n",
    "\n",
    "1.  **Apply Filters:** Optionally filter data further based on qualification types (e.g., only Ammattitutkinto, Erikoisammattitutkinto) if `ANALYSIS_PARAMS['filter_qual_types']` is `True`.\n",
    "2.  **Initialize & Run:** Create an instance of `MarketAnalyzer` and call its `analyze()` method.\n",
    "3.  **Display Results:** Show samples (top rows) of the resulting analysis DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap analysis step in try-except\n",
    "analysis_results = {}\n",
    "analyzer = None\n",
    "\n",
    "try:\n",
    "    logger.info(\"--- Step 2: Performing Market Analysis ---\")\n",
    "    # Ensure data from Step 1 is available\n",
    "    if 'df_prepared' not in locals() or df_prepared is None or df_prepared.empty:\n",
    "        logger.warning(\"Prepared data (df_prepared) is missing or empty. Skipping Market Analysis.\")\n",
    "        # Allow execution to continue, analysis_results remains empty\n",
    "    else:\n",
    "        # --- 2a: Apply Optional Qualification Type Filtering ---
",
    "        if filter_qual_types_flag: # Variable from Step 1\n",
    "            qual_types = config.get('qualification_types', ['Ammattitutkinnot', 'Erikoisammattitutkinnot']) # Default types if not in config\n",
    "            logger.info(f\"Applying qualification type filter for: {qual_types}\")\n",
    "            df_analysis_input = df_prepared[df_prepared['tutkintotyyppi'].isin(qual_types)].copy()\n",
    "            logger.info(f\"Shape before type filter: {df_prepared.shape}, after: {df_analysis_input.shape}\")\n",
    "        else:\n",
    "            logger.info(\"Skipping qualification type filtering.\")\n",
    "            df_analysis_input = df_prepared.copy() # Use the prepared data directly\n",
    "\n",
    "        # --- 2b: Initialize and Run Analyzer ---
",
    "        logger.info(\"Initializing MarketAnalyzer...\")\n",
    "        analyzer = MarketAnalyzer(df_analysis_input, cfg=config)\n",
    "        analyzer.institution_names = institution_variants # From Step 1\n",
    "        analyzer.institution_short_name = institution_short_name # From Step 1\n",
    "        logger.info(f\"Analyzer configured for: {analyzer.institution_short_name} ({analyzer.institution_names})\")\n",
    "\n",
    "        logger.info(\"Running analysis...\")\n",
    "        analysis_results = analyzer.analyze() # This returns a dict of DataFrames\n",
    "        logger.info(f\"Analysis complete. Results keys: {list(analysis_results.keys())}\")\n",
    "\n",
    "        # --- 2c: Display Sample Results ---
",
    "        # Print head of key results DataFrames for notebook-like inspection\n",
    "        print(\"\n--- Analysis Results Samples ---\")\n",
    "        from IPython.display import display\n",
    "        for key, df_result in analysis_results.items():\n",
    "            if isinstance(df_result, pd.DataFrame) and not df_result.empty:\n",
    "                print(f\"\n--- Result: {key} (Top 5 rows) ---\")\n",
    "                display(df_result.head())\n",
    "            elif isinstance(df_result, pd.DataFrame) and df_result.empty:\n",
    "                 print(f\"\n--- Result: {key} (DataFrame is empty) ---\")\n",
    "            else:\n",
    "                # Handle non-DataFrame results if any (e.g., scalars, lists)\n",
    "                 print(f\"\n--- Result: {key} (Type: {type(df_result)}) ---\")\n",
    "                 print(df_result)\n",
    "                 \n",
    "        logger.info(\"--- Step 2: Market Analysis Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during market analysis: {e}\", exc_info=True)\n",
    "    # analysis_results remains {} or its last value, analyzer remains None or its last value\n",
    "    # sys.exit(1) -> Remove exit\n",
    "    raise # Re-raise to indicate failure\n",
    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate Visualizations (PDF Report)\n",
    "\n",
    "This step uses the `EducationVisualizer` to generate plots based on the analysis results and saves them into a single PDF report.\n",
    "\n",
    "1.  **Initialize Visualizer:** Set up the visualizer for PDF output.\n",
    "2.  **Determine Active Qualifications:** Identify qualifications meeting activity criteria for focused plots.\n",
    "3.  **Generate Plots:** Create standard plots (Volume Area, Market Share Lines, Share Heatmap, BCG Matrix, Volume/Provider Count) using specific visualizer methods.\n",
    "4.  **Save to PDF:** Each generated plot figure is added as a page to the PDF file.\n",
    "5.  **Finalize PDF:** The PDF file is closed and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap visualization step in try-except\n",
    "pdf_report_path = None # Store path to the generated PDF\n",
    "visualizer = None # Define visualizer in outer scope\n",
    "active_qualifications = [] # Define active_qualifications in outer scope\n",
    "\n",
    "try:\n",
    "    logger.info(\"--- Step 3: Generating Visualizations ---\")\n",
    "\n",
    "    # Ensure results from Step 2 are available\n",
    "    if 'analysis_results' not in locals() or not analysis_results or 'analyzer' not in locals() or analyzer is None:\n",
    "        logger.warning(\"Analysis results or analyzer instance missing. Skipping Visualizations.\")\n",
    "    # Also ensure necessary variables from Step 1 are available for titles/paths\n",
    "    elif 'institution_short_name' not in locals() or institution_short_name is None or \'data_update_date_str' not in locals():\n",
    "         logger.warning(\"Institution short name or data update date missing from Step 1. Skipping Visualizations.\")\n",
    "    else:\n",
    "        # Determine base output directory\n",
    "        base_output_path_str = ANALYSIS_PARAMS.get('output_dir') or config.get('paths', {}).get('output', 'output') # Default to 'output'\n",
    "        base_output_path = Path(base_output_path_str)\n",
    "\n",
    "        # Create the specific subdirectory for the institution's report (Excel and PDF)\n",
    "        report_dir_name = f\"education_market_{institution_short_name.lower()}\"\n",
    "        report_output_dir = base_output_path / report_dir_name\n",
    "        report_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"PDF report will be saved in directory: {report_output_dir}\")\n",
    "\n",
    "        # Initialize Visualizer - For PDF Output\n",
    "        logger.info(\"Initializing EducationVisualizer for PDF output...\")\n",
    "        visualizer = EducationVisualizer(\n",
    "            style=\"default\",\n",
    "            output_dir=report_output_dir, # Use the main report dir\n",
    "            output_format='pdf', # Generate PDF\n",
    "            institution_short_name=institution_short_name,\n",
    "            include_timestamp=ANALYSIS_PARAMS.get('include_timestamp', True) # Match Excel timestamping\n",
    "        )\n",
    "        visualizer.data_update_date = data_update_date_str # Pass date string from Step 1\n",
    "\n",
    "        logger.info(\"Generating plots and adding them to PDF...\")\n",
    "\n",
    "        # --- Get Config Column Names (using defaults as fallback) ---
",
    "        cols_out = config.get('columns', {}).get('output', {})
",
    "        year_col = cols_out.get('year', 'Vuosi')
",
    "        qual_col = cols_out.get('qualification', 'Tutkinto')
",
    "        provider_col = cols_out.get('provider', 'Oppilaitos')
",
    "        provider_amount_col = cols_out.get('provider_amount', 'NOM järjestäjänä')
",
    "        subcontractor_amount_col = cols_out.get('subcontractor_amount', 'NOM hankintana')
",
    "        total_volume_col = cols_out.get('total_volume', 'NOM yhteensä')
",
    "        market_total_col = cols_out.get('market_total', 'Markkina yhteensä')
",
    "        market_share_col = cols_out.get('market_share', 'Markkinaosuus (%)')
",
    "        bcg_growth_col = 'Market Growth (%)'
",
    "        bcg_share_col = 'Relative Market Share'
",
    "        bcg_size_col = 'Institution Volume'
",
    "        count_provider_col = 'Unique_Providers_Count'
",
    "        count_subcontractor_col = 'Unique_Subcontractors_Count'
",
    "\n",
    "        # --- Get other needed info ---
",
    "        inst_short_name = analyzer.institution_short_name
",
    "        inst_names = analyzer.institution_names
",
    "        min_year = analyzer.min_year
",
    "        max_year = analyzer.max_year
",
    "        base_caption = TEXT_CONSTANTS[\"data_source\"].format(date=data_update_date_str)
",
    "        last_full_year = max_year - 1 if max_year and min_year and max_year > min_year else max_year
",
    "        plot_reference_year = last_full_year if last_full_year else max_year\n",
    "\n",
    "        # --- Plot 1: Stacked Area (Total Volumes) ---
",
    "        total_volumes_df = analysis_results.get('total_volumes')
",
    "        if total_volumes_df is not None and not total_volumes_df.empty and all(c in total_volumes_df.columns for c in [year_col, provider_amount_col, subcontractor_amount_col]):
",
    "            try:\n",
    "                logger.info(\"Generating Total Volumes Area Chart...\")\n",
    "                plot_df_roles = total_volumes_df.rename(columns={provider_amount_col: 'järjestäjänä', subcontractor_amount_col: 'hankintana'})\n",
    "                fig, _ = visualizer.create_area_chart(\n",
    "                    data=plot_df_roles, x_col=year_col, y_cols=['järjestäjänä', 'hankintana'],\n",
    "                    colors=[COLOR_PALETTES[\"roles\"][\"järjestäjänä\"], COLOR_PALETTES[\"roles\"][\"hankintana\"]],\n",
    "                    labels=['järjestäjänä', 'hankintana'], title=f\"{inst_short_name} netto-opiskelijamäärä vuosina {min_year}-{max_year}\",\n",
    "                    caption=base_caption, stacked=True\n",
    "                )\n",
    "                # Add figure to PDF\n",
    "                visualizer.save_visualization(fig, f\"{inst_short_name}_total_volumes_area\")\n",
    "                plt.close(fig)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Total Volumes plot: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(f\"Skipping Total Volumes plot: Data not available or missing columns.\")\n",
    "\n",
    "        # --- Determine Active Qualifications (logic from analyze_cli.py) ---
",
    "        detailed_df = analysis_results.get('detailed_providers_market')\n",
    "        # active_qualifications = [] # Defined in outer scope\n",
    "        if detailed_df is not None and not detailed_df.empty and max_year is not None and min_year is not None:\n",
    "            years_to_check_activity = [last_full_year]\n",
    "            prev_full_year = last_full_year - 1 if last_full_year > min_year else None\n",
    "            if prev_full_year: years_to_check_activity.append(prev_full_year)\n",
    "\n",
    "            inst_recent_df = detailed_df[(detailed_df[provider_col].isin(inst_names)) & (detailed_df[year_col].isin(years_to_check_activity))].copy()\n",
    "            analysis_config = config.get('analysis', {})
",
    "            min_volume_sum_threshold = analysis_config.get('active_qualification_min_volume_sum', 3)\n",
    "            quals_with_recent_volume = set()\n",
    "            if total_volume_col in inst_recent_df.columns:\n",
    "                volume_grouped = inst_recent_df.groupby(qual_col)[total_volume_col].sum()\n",
    "                quals_with_recent_volume = set(volume_grouped[volume_grouped >= min_volume_sum_threshold].index)\n",
    "            \n",
    "            quals_with_100_share_both_years = set()\n",
    "            if prev_full_year is not None and not inst_recent_df.empty and market_share_col in inst_recent_df.columns:\n",
    "                inst_recent_df[market_share_col] = pd.to_numeric(inst_recent_df[market_share_col], errors='coerce')\n",
    "                inst_100_share_df = inst_recent_df[inst_recent_df[market_share_col].round(2) == 100.0]\n",
    "                share_counts = inst_100_share_df.groupby(qual_col)[year_col].nunique()\n",
    "                quals_with_100_share_both_years = set(share_counts[share_counts == 2].index)\n",
    "                \n",
    "            active_qualifications_set = quals_with_recent_volume - quals_with_100_share_both_years\n",
    "            active_qualifications = sorted(list(active_qualifications_set))\n",
    "            logger.info(f\"Determined {len(active_qualifications)} active qualifications for plots.\")\n",
    "        else:\n",
    "            logger.warning(\"Could not determine active qualifications. Using fallback or skipping related plots.\")\n",
    "            if detailed_df is not None and not detailed_df.empty and provider_col in detailed_df.columns and qual_col in detailed_df.columns:\n",
    "                 active_qualifications = detailed_df[detailed_df[provider_col].isin(inst_names)][qual_col].unique().tolist()\n",
    "                 logger.info(f\"Using fallback: {len(active_qualifications)} qualifications institution ever offered.\")\n",
    "\n",
    "        # --- Plot 2: Line Chart (Market Share Evolution) - Loop per Qualification ---
",
    "        if detailed_df is not None and not detailed_df.empty:\n",
    "            logger.info(f\"Generating Market Share Line Charts for {len(active_qualifications)} active qualifications...\")\n",
    "            for qual in active_qualifications:\n",
    "                try:\n",
    "                    qual_df = detailed_df[detailed_df[qual_col] == qual]\n",
    "                    if qual_df.empty: continue\n",
    "                    latest_qual_providers = qual_df[qual_df[year_col] == plot_reference_year]\n",
    "                    top_m_providers = latest_qual_providers.nlargest(6, market_share_col)[provider_col].tolist()\n",
    "                    plot_data = qual_df[qual_df[provider_col].isin(top_m_providers)].pivot(index=year_col, columns=provider_col, values=market_share_col)\n",
    "                    if not plot_data.empty:\n",
    "                        plot_data.index.name = year_col # Ensure index name\n",
    "                        qual_filename_part = qual.replace(' ', '_').replace('/', '_').replace(':', '_').replace(',', '').replace('.', '').lower()[:50] # More robust filename part\n",
    "                        fig, _ = visualizer.create_line_chart(\n",
    "                            data=plot_data, x_col=plot_data.index, y_cols=top_m_providers,\n",
    "                            colors=COLOR_PALETTES[\"main\"], labels=top_m_providers,\n",
    "                            title=f\"{qual}: Markkinaosuus (%)\", caption=base_caption, markers=True\n",
    "                        )\n",
    "                        # Add figure to PDF\n",
    "                        visualizer.save_visualization(fig, f\"{inst_short_name}_{qual_filename_part}_market_share_lines\")\n",
    "                        plt.close(fig)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to generate Market Share line plot for {qual}: {e}\", exc_info=True)\n",
    "                    if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping Market Share Line Charts: Detailed market data missing.\")\n",
    "\n",
    "        # --- Plot 3: Heatmap (Institution's Share) ---
",
    "        if detailed_df is not None and not detailed_df.empty:\n",
    "            try:\n",
    "                logger.info(\"Generating Institution Market Share Heatmap...\")\n",
    "                inst_share_df_raw = detailed_df[detailed_df[provider_col].isin(inst_names)].copy() # Use .copy()\n",
    "                inst_share_df = inst_share_df_raw # Start with raw, aggregate if multiple variants\n",
    "                \n",
    "                if len(inst_names) > 1 and not inst_share_df_raw.empty:\n",
    "                     # Define aggregation logic using config column names\n",
    "                    agg_logic = {\n",
    "                        provider_amount_col: 'sum', subcontractor_amount_col: 'sum',\n",
    "                        total_volume_col: 'sum', market_total_col: 'first',\n",
    "                        market_share_col: 'first', # Placeholder, recalculate\n",
    "                        # Add other columns if needed, e.g., 'first' for rank\n",
    "                    }\n",
    "                    agg_logic = {k: v for k, v in agg_logic.items() if k in inst_share_df_raw.columns}\n",
    "                    if agg_logic:\n",
    "                        inst_share_df_agg = inst_share_df_raw.groupby([year_col, qual_col], as_index=False).agg(agg_logic)\n",
    "                        # Recalculate market share\n",
    "                        if total_volume_col in inst_share_df_agg.columns and market_total_col in inst_share_df_agg.columns:\n",
    "                            valid_market_total = inst_share_df_agg[market_total_col] > 0\n",
    "                            inst_share_df_agg[market_share_col] = 0.0\n",
    "                            inst_share_df_agg.loc[valid_market_total, market_share_col] = (inst_share_df_agg.loc[valid_market_total, total_volume_col] / inst_share_df_agg.loc[valid_market_total, market_total_col] * 100)\n",
    "                        inst_share_df = inst_share_df_agg\n",
    "                    else:\n",
    "                         logger.warning(\"Aggregation skipped for heatmap: No relevant columns found for aggregation.\")\n",
    "\n",
    "                if not inst_share_df.empty:\n",
    "                    inst_share_df_active = inst_share_df[inst_share_df[qual_col].isin(active_qualifications)]\n",
    "                else: inst_share_df_active = pd.DataFrame() # Ensure it's an empty df\n",
    "                \n",
    "                if not inst_share_df_active.empty and market_share_col in inst_share_df_active.columns:\n",
    "                    heatmap_data = inst_share_df_active.pivot_table(index=qual_col, columns=year_col, values=market_share_col)\n",
    "                    heatmap_data = heatmap_data.sort_index() # Sort rows\n",
    "                    if not heatmap_data.empty:\n",
    "                        fig, _ = visualizer.create_heatmap(\n",
    "                            data=heatmap_data, title=f\"{inst_short_name} markkinaosuus (%) aktiivisissa tutkinnoissa\",\n",
    "                            caption=base_caption, cmap=\"Greens\", annot=True, fmt=\".1f\"\n",
    "                        )\n",
    "                        # Add figure to PDF\n",
    "                        visualizer.save_visualization(fig, f\"{inst_short_name}_market_share_heatmap_active\")\n",
    "                        plt.close(fig)\n",
    "                    else: logger.warning(f\"Skipping Heatmap: Pivoted data is empty for active qualifications.\")\n",
    "                else: logger.warning(f\"Skipping Heatmap: No data for {inst_short_name} in active qualifications or missing market share column.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Market Share Heatmap: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping Heatmap: Detailed market data missing.\")\n",
    "\n",
    "        # --- Plot 4: BCG Matrix ---
",
    "        bcg_data_df = analysis_results.get('bcg_data')\n",
    "        if bcg_data_df is not None and not bcg_data_df.empty:\n",
    "            logger.info(\"Generating BCG Growth-Share Matrix...\")\n",
    "            try:\n",
    "                required_bcg_cols = [qual_col, bcg_growth_col, bcg_share_col, bcg_size_col]\n",
    "                # Check if qual_col from config is present (it might be named differently initially)\n",
    "                actual_qual_col = qual_col if qual_col in bcg_data_df.columns else 'Qualification' # Check common alternative\n",
    "                if actual_qual_col not in bcg_data_df.columns: \n",
    "                    logger.warning(f\"BCG Matrix: Qualification column ('{qual_col}' or 'Qualification') not found.\")\n",
    "                elif all(c in bcg_data_df.columns for c in [bcg_growth_col, bcg_share_col, bcg_size_col]):\n",
    "                    plot_title = f\"{inst_short_name}: Tutkintojen kasvu vs. markkinaosuus ({plot_reference_year})\"\n",
    "                    bcg_caption = base_caption + f\" Kuplan koko = {inst_short_name} volyymi ({plot_reference_year}). Suhteellinen markkinaosuus = {inst_short_name} osuus / Suurimman kilpailijan osuus.\"\n",
    "                    fig, _ = visualizer.create_bcg_matrix(\n",
    "                        data=bcg_data_df, growth_col=bcg_growth_col, share_col=bcg_share_col,\n",
    "                        size_col=bcg_size_col, label_col=actual_qual_col, title=plot_title, caption=bcg_caption\n",
    "                    )\n",
    "                    # Add figure to PDF\n",
    "                    visualizer.save_visualization(fig, f\"{inst_short_name}_bcg_matrix\")\n",
    "                    plt.close(fig)\n",
    "                else: logger.warning(f\"Skipping BCG Matrix: Missing required columns. Found: {bcg_data_df.columns.tolist()}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate BCG Matrix plot: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping BCG Matrix: bcg_data not available or empty.\")\n",
    "\n",
    "        # --- Plot 5: Combined Volume / Provider Count Plot ---
",
    "        volume_df = analysis_results.get('total_volumes')\n",
    "        count_df = analysis_results.get('provider_counts_by_year')\n",
    "        if volume_df is not None and not volume_df.empty and count_df is not None and not count_df.empty:\n",
    "            logger.info(\"Generating Volume / Provider Count Plot...\")\n",
    "            try:\n",
    "                # Check if all required columns exist\n",
    "                req_vol_cols = [year_col, provider_amount_col, subcontractor_amount_col]\n",
    "                req_count_cols = [year_col, count_provider_col, count_subcontractor_col]\n",
    "                if all(c in volume_df.columns for c in req_vol_cols) and all(c in count_df.columns for c in req_count_cols):\n",
    "                    plot_title = f\"{inst_short_name}: Opiskelijamäärät ja kouluttajamarkkina ({min_year}-{max_year})\"\n",
    "                    plot_caption = base_caption + f\". Kouluttajamäärä perustuu tutkintoihin, joita {inst_short_name} tarjoaa.\"\n",
    "                    fig, _ = visualizer.create_volume_and_provider_count_plot(\n",
    "                        volume_data=volume_df, count_data=count_df, title=plot_title,\n",
    "                        volume_title=\"Netto-opiskelijamäärä\", count_title=\"Uniikit kouluttajat markkinassa\",\n",
    "                        year_col=year_col, vol_provider_col=provider_amount_col, vol_subcontractor_col=subcontractor_amount_col,\n",
    "                        count_provider_col=count_provider_col, count_subcontractor_col=count_subcontractor_col,\n",
    "                        caption=plot_caption\n",
    "                    )\n",
    "                    # Add figure to PDF\n",
    "                    visualizer.save_visualization(fig, f\"{inst_short_name}_volume_provider_counts\")\n",
    "                    plt.close(fig)\n",
    "                else:\n",
    "                    logger.warning(f\"Skipping Volume/Provider Count plot: Missing required columns. Vol needed: {req_vol_cols} (found {volume_df.columns.tolist()}). Count needed: {req_count_cols} (found {count_df.columns.tolist()})\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Volume/Provider Count plot: {e}\", exc_info=True)\n",
    "                if 'fig' in locals() and plt.fignum_exists(fig.number): plt.close(fig)\n",
    "        else: logger.warning(\"Skipping Volume/Provider Count plot: Required volume or count data missing.\")\n",
    "\n",
    "        # --- Finalize and Close PDF ---
",
    "        # Check if visualizer was initialized before closing\n",
    "        if visualizer is not None:\n",
    "            logger.info(\"Finalizing PDF report...\")\n",
    "            visualizer.close_pdf() # Close the PDF file\n",
    "            pdf_report_path = visualizer.pdf_path # Get the path where the PDF was saved\n",
    "            if pdf_report_path:\n",
    "                 logger.info(f\"PDF report saved to: {pdf_report_path}\")\n",
    "            else:\n",
    "                 logger.warning(\"Could not determine PDF report path from visualizer.\")\n",
    "        else:\n",
    "             logger.warning(\"Visualizer was not initialized, cannot close PDF.\")\n",
    "\n",
    "        logger.info(f\"--- Step 3: PDF Report Generation Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during PDF visualization generation: {e}\", exc_info=True)\n",
    "    pdf_report_path = None # Ensure path is None on error\n",
    "    # If visualizer exists and has an open PDF handle, try to close it to avoid locking file\n",
    "    if visualizer is not None and hasattr(visualizer, 'pdf_pages') and visualizer.pdf_pages is not None:\n",
    "        try:\n",
    "            logger.warning(\"Attempting to close PDF due to error during generation...\")\n",
    "            visualizer.close_pdf()\n",
    "        except Exception as close_err:\n",
    "            logger.error(f\"Error closing PDF after visualization error: {close_err}\", exc_info=True)\n",
    "    raise # Re-raise outer exception\n",
    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export Final Results (Excel)\n",
    "\n",
    "This step exports the analysis results DataFrames into a multi-sheet Excel file.\n",
    "\n",
    "1.  **Create Metadata:** Generate a DataFrame containing metadata about the analysis run.\n",
    "2.  **Export:** Call the `export_analysis_results` function (from `analyze_cli`) which handles file naming, directory creation, and saving the data and metadata to the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap export step in try-except\n",
    "excel_path = None # Initialize path\n",
    "metadata_df = None\n",
    "\n",
    "try:\n",
    "    logger.info(\"--- Step 4: Exporting Final Results to Excel ---\")\n",
    "\n",
    "    # Ensure analysis results are available\n",
    "    if 'analysis_results' not in locals() or not analysis_results:\n",
    "        logger.warning(\"Analysis results missing. Skipping Excel Export.\")\n",
    "    # Also ensure necessary variables from Step 1 are available\n",
    "    elif 'institution_name' not in locals() or institution_name is None or \'institution_short_name' not in locals() or institution_short_name is None or \n",
    "         'institution_variants' not in locals() or not institution_variants or 'data_file_path' not in locals() or data_file_path is None or \n",
    "         'data_update_date_str' not in locals() or 'filter_qual_types_flag' not in locals():\n",
    "         logger.warning(\"One or more required parameters from Step 1 are missing or invalid. Skipping Excel Export.\")\n",
    "    else:\n",
    "        # --- Create Metadata ---
",
    "        logger.info(\"Creating metadata for export...\")\n",
    "        metadata = {\n",
    "            \"Analysis Timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"Institution Analyzed\": f\"{institution_name} ({institution_short_name})\", # Use variables from Step 1\n",
    "            \"Institution Variants Used\": \", \".join(institution_variants),\n",
    "            \"Input Data File\": data_file_path,\n",
    "            \"Data Update Date\": data_update_date_str,\n",
    "            \"Qualification Type Filter Applied\": \"Yes\" if filter_qual_types_flag else \"No\",\n",
    "            \"Min Market Size Threshold (for plots)\": config.get('analysis', {}).get('min_market_size_threshold', 'N/A'),\n",
    "            \"Active Qualifications Filter Threshold (for plots)\": config.get('analysis', {}).get('active_qualification_min_volume_sum', 'N/A')\n",
    "        }\n",
    "        metadata_df = pd.DataFrame(metadata.items(), columns=[\"Parameter\", \"Value\"])\n",
    "        logger.info(\"Metadata created successfully.\")\n",
    "        print(\"\n--- Analysis Metadata ---\")\n",
    "        from IPython.display import display\n",
    "        display(metadata_df)\n",
    "        \n",
    "        # --- Determine Base Output Path ---
",
    "        base_output_path_str = ANALYSIS_PARAMS.get('output_dir') or config.get('paths', {}).get('output', 'output') # Default to 'output'\n",
    "        base_output_path = Path(base_output_path_str)\n",
    "        logger.info(f\"Using base output directory for Excel export: {base_output_path}\")\n",
    "\n",
    "        # --- Call Exporter (using the wrapper from analyze_cli) ---
",
    "        excel_path = export_analysis_results(\n",
    "            analysis_results=analysis_results,\n",
    "            config=config,\n",
    "            institution_short_name=institution_short_name,\n",
    "            base_output_path=str(base_output_path), # Pass as string\n",
    "            metadata_df=metadata_df,\n",
    "            include_timestamp=ANALYSIS_PARAMS.get('include_timestamp', True)\n",
    "        )\n",
    "        \n",
    "        if excel_path:\n",
    "            logger.info(f\"Successfully exported results to Excel: {excel_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Excel export function did not return a valid path.\")\n",
    "            \n",
    "        logger.info(\"--- Step 4: Excel Export Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during Excel export: {e}\", exc_info=True)\n",
    "    excel_path = None # Ensure path is None on error\n",
    "    # sys.exit(1) -> Remove exit\n",
    "    raise # Re-raise to indicate failure\n",
    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Custom User Analysis Area\n",
    "\n",
    "The main analysis workflow is complete. Key variables available for further analysis in subsequent cells include:\n",
    "\n",
    "*   `config`: The loaded project configuration dictionary.\n",
    "*   `ANALYSIS_PARAMS`: The parameters used for this specific run.\n",
    "*   `df_raw`: The raw data loaded initially.\n",
    "*   `df_prepared`: The cleaned and filtered data used for the main analysis.\n",
    "*   `analysis_results`: A dictionary containing the various analysis DataFrames.\n",
    "    *   Keys: `['total_volumes', 'volumes_by_qualification', 'detailed_providers_market', 'qualification_cagr', 'overall_total_market_volume', 'qualification_market_yoy_growth', 'provider_counts_by_year', 'bcg_data']` (or similar, depending on analysis run)\n",
    "*   `analyzer`: The `MarketAnalyzer` instance.\n",
    "*   `active_qualifications`: List of qualifications identified as active for plotting.\n",
    "*   `pdf_report_path`: Path to the generated PDF report (if successful).\n",
    "*   `excel_path`: The path to the generated Excel file (if export was successful).\n",
    "*   `institution_short_name`, `institution_variants`, `data_update_date_str`\n",
    "\n",
    "You can now add your custom Python code in new cells below to explore these results further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Accessing the detailed market data DataFrame\n",
    "logger.info(\"Example of accessing results for custom analysis:\")\n",
    "\n",
    "if 'analysis_results' in locals() and 'detailed_providers_market' in analysis_results:\n",
    "    print(\"\n--- Detailed Providers Market Sample (from analysis_results) ---\")\n",
    "    detailed_df_example = analysis_results['detailed_providers_market']\n",
    "    from IPython.display import display\n",
    "    display(detailed_df_example.head())\n",
    "else:\n",
    "    print(\"Detailed market data not available for display.\")\n",
    "\n",
    "print(\"\n--- Available Variables for Custom Analysis ---\")\n",
    "# Use try-except for potentially undefined variables if prior steps failed\n",
    "try:\n",
    "    print(f\"{pdf_report_path=}\")\n",
    "except NameError:\n",
    "    print(\"pdf_report_path is not defined (visualization step may have failed)\")\n",
    "try:\n",
    "    print(f\"{excel_path=}\")\n",
    "except NameError:\n",
    "    print(\"excel_path is not defined (Excel export step may have failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- End of Standard Workflow ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 