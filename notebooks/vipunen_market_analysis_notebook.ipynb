{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6959f364",
   "metadata": {},
   "source": [
    "# Vipunen Education Market Analysis Notebook\n",
    "\n",
    "This script replicates the analysis workflow from the Vipunen project,\n",
    "displaying intermediate results and plots in a way suitable for notebooks\n",
    "or interactive environments, before exporting the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2288a266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "from IPython.display import display \n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "import sys \n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b7b586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path (for local environment)\n",
    "project_root = str(Path().resolve().parent)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf31e45",
   "metadata": {},
   "source": [
    "#### Project Module Imports\n",
    "Assuming the script is run from the project root or the environment includes the src path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03adb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.vipunen.config.config_loader import get_config\n",
    "    from src.vipunen.data.data_loader import load_data\n",
    "    from src.vipunen.data.data_processor import clean_and_prepare_data\n",
    "    from src.vipunen.export.excel_exporter import export_to_excel\n",
    "    from src.vipunen.analysis.market_analyzer import MarketAnalyzer\n",
    "    # Import the wrapper function from analyze_cli\n",
    "    from src.vipunen.cli.analyze_cli import export_analysis_results\n",
    "    # Import Visualizer and constants if needed later\n",
    "    from src.vipunen.visualization.education_visualizer import EducationVisualizer, COLOR_PALETTES, TEXT_CONSTANTS\n",
    "    from src.vipunen.utils.data_utils import extract_data_update_date\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Ensure the script is run from the project root or 'src' is in the Python path.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717906a9",
   "metadata": {},
   "source": [
    "### Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7f29e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO, # Use INFO level for notebook clarity, DEBUG can be verbose\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    # Force logging to stdout for notebook-like output\n",
    "    stream=sys.stdout \n",
    ")\n",
    "\n",
    "#Silence overly verbose loggers\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"FileUtils\").setLevel(logging.WARNING) \n",
    "\n",
    "logger = logging.getLogger(\"AnalysisNotebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f492d016",
   "metadata": {},
   "source": [
    "## Analysis Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e03a09",
   "metadata": {},
   "source": [
    "### Load main project config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03f5fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Setting Analysis Configuration ---\")\n",
    "try:\n",
    "    config = get_config() # Assumes config.yaml is discoverable\n",
    "    logger.info(\"Successfully loaded project configuration.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load project configuration: {e}. Exiting.\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a3886",
   "metadata": {},
   "source": [
    "#### Define analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab1cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# institution = \"AEL-Amiedu Oy\"\n",
    "# institution_variants = [\"Taitotalo\", \"Ami-säätiö sr\",\"Ammattienedistämislaitossäätiö AEL sr\"]\n",
    "# institution_short_name = \"Taitotalo\"\n",
    "\n",
    "institution = \"Rastor-instituutti ry\"\n",
    "institution_variants = [\"Rastor Oy\"]\n",
    "institution_short_name = \"RI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840d0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANALYSIS_PARAMS = {\n",
    "    'data_file': \"amm_opiskelijat_ja_tutkinnot_vuosi_tutkinto.csv\", # Set to None to use path from config, or provide a specific path string\n",
    "    'institution': institution, # Can be a key from config.yaml (e.g., 'default', 'ael_amiedu') OR the full institution name string.\n",
    "    'short_name': None, # DEPRECATED - Use institution_short_name below. Set to None to use default from config based on institution\n",
    "    'institution_short_name': institution_short_name, # Optional: Provide a short name if 'institution' is not a key in config.yaml. Defaults to the full name.\n",
    "    'institution_variants': institution_variants, # Optional: Provide a list of name variants if 'institution' is not a key in config.yaml. Example: [\"Amiedu\", \"AEL\"]\n",
    "    'use_dummy': False, # Set to True to use dummy data if available\n",
    "    'filter_qual_types': False, # Set to True to filter for specific qualification types (e.g., Ammatti-/Erikoisammattitutkinto)\n",
    "    'output_dir': None, # Set to None to use path from config, or provide specific base dir for outputs\n",
    "    'include_timestamp': True # Whether to include timestamp in output filenames\n",
    "    # Add other parameters as needed, mirroring analyze_cli args\n",
    "}\n",
    "logger.info(f\"Using Analysis Parameters: {ANALYSIS_PARAMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faceb62c",
   "metadata": {},
   "source": [
    "#### Extract key config sections (config.yaml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24783c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = config.get('columns', {}).get('input', {})\n",
    "output_cols = config.get('columns', {}).get('output', {})\n",
    "excel_config = config.get('excel', {})\n",
    "analysis_config = config.get('analysis', {})\n",
    "if not input_cols:\n",
    "    logger.warning(\"Input column mapping ('columns.input') not found or empty in config.\")\n",
    "# Add similar checks for output_cols, excel_config if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf9dc3",
   "metadata": {},
   "source": [
    "## Main Analysis Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3927ff2a",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Data (Load, Clean, Filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a8bfe",
   "metadata": {},
   "source": [
    "Setup & Parameter Resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Resolve Data File Path ---\n",
    "data_file_param = ANALYSIS_PARAMS.get('data_file')\n",
    "root_path = Path(project_root) # Define root_path object early\n",
    "\n",
    "if data_file_param:\n",
    "    data_file = Path(data_file_param)\n",
    "    logger.info(f\"Using data file specified in ANALYSIS_PARAMS: {data_file}\")\n",
    "else:\n",
    "    data_file = root_path / config.get('paths', {}).get('data', 'data/raw/default.csv') # Use root_path for default data path too\n",
    "    logger.info(f\"Using default data file from config (relative to project root): {data_file}\")\n",
    "\n",
    "use_dummy = ANALYSIS_PARAMS.get('use_dummy', False)\n",
    "filter_qual_types_flag = ANALYSIS_PARAMS.get('filter_qual_types', False)\n",
    "\n",
    "# Ensure the data file exists (optional but good practice)\n",
    "# if not data_file.exists():\n",
    "#     err_msg = f\"Data file not found: {data_file.resolve()}\"\n",
    "#     logger.error(err_msg)\n",
    "#     # In a notebook, raising an error is better than sys.exit\n",
    "#     raise FileNotFoundError(err_msg)\n",
    "\n",
    "institution_param = ANALYSIS_PARAMS.get('institution', 'default') # Default to 'default' key if not specified\n",
    "institution_config = config.get('institutions', {})\n",
    "\n",
    "if institution_param in institution_config:\n",
    "    logger.info(f\"Using institution configuration for key: '{institution_param}'\")\n",
    "    inst_conf = institution_config[institution_param]\n",
    "    institution_name = inst_conf.get('name', institution_param) # Default name to key if missing\n",
    "    institution_short_name = ANALYSIS_PARAMS.get('institution_short_name') or inst_conf.get('short_name', institution_name) # Param overrides config, defaults to name\n",
    "    institution_variants = ANALYSIS_PARAMS.get('institution_variants') or inst_conf.get('variants', []) # Param overrides config\n",
    "else:\n",
    "    logger.info(f\"Using institution name directly from ANALYSIS_PARAMS: '{institution_param}'\")\n",
    "    institution_name = institution_param\n",
    "    institution_short_name = ANALYSIS_PARAMS.get('institution_short_name', institution_name) # Default short name to full name if not provided\n",
    "    institution_variants = ANALYSIS_PARAMS.get('institution_variants', []) # Get variants from params, default to empty list\n",
    "    # Ensure institution_variants is a list\n",
    "    if not isinstance(institution_variants, list):\n",
    "        logger.warning(f\"'institution_variants' in ANALYSIS_PARAMS was not a list. Please provide a list of strings. Ignoring the provided value.\")\n",
    "        institution_variants = [] # Default to empty list if not a list\n",
    "    elif not institution_variants: # Check if the list is empty after ensuring it's a list\n",
    "        logger.warning(f\"No 'institution_variants' provided in ANALYSIS_PARAMS for '{institution_name}'. Matching might be less robust if data uses slightly different names.\")\n",
    "\n",
    "# Ensure the main name is always included in the list of names to check in the data\n",
    "institution_names_to_match = list(set([institution_name] + institution_variants)) # Use set to avoid duplicates\n",
    "\n",
    "logger.info(f\"Analyzing institution: '{institution_name}' (Short: '{institution_short_name}')\")\n",
    "logger.info(f\"Will match data against names: {institution_names_to_match}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c743788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Resolve Output Directory ---\n",
    "output_base_dir_param = ANALYSIS_PARAMS.get('output_dir')\n",
    "if output_base_dir_param:\n",
    "    # If user provides a path, resolve it relative to the current working directory\n",
    "    output_base_dir = Path(output_base_dir_param)\n",
    "    logger.info(f\"Using output directory specified in ANALYSIS_PARAMS: {output_base_dir.resolve()}\")\n",
    "else:\n",
    "    # If using default from config, resolve it relative to the project root\n",
    "    relative_output_path = config.get('paths', {}).get('output', 'data/reports') # Get relative path, default if missing\n",
    "    # root_path is already defined above\n",
    "    output_base_dir = root_path / relative_output_path\n",
    "    logger.info(f\"Using default output directory from config (relative to project root): {output_base_dir.resolve()}\")\n",
    "\n",
    "output_base_dir.mkdir(parents=True, exist_ok=True) # Ensure base directory exists\n",
    "# logger.info(f\"Using output base directory: {output_base_dir.resolve()}\") # Logged within the if/else block now\n",
    "\n",
    "# --- Resolve Filename Timestamp ---\n",
    "include_timestamp = ANALYSIS_PARAMS.get('include_timestamp', True)\n",
    "timestamp_str = f\"_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\" if include_timestamp else \"\"\n",
    "\n",
    "logger.info(f\"Using Input Columns: {input_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb48961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construct Output Filenames ---\n",
    "# Use institution_short_name for brevity in filenames\n",
    "filename_base = f\"{institution_short_name}_analysis{timestamp_str}\"\n",
    "excel_filename = output_base_dir / f\"{filename_base}.xlsx\"\n",
    "pdf_filename = output_base_dir / f\"{filename_base}.pdf\" # Assuming PDF is still desired\n",
    "# png_dir = output_base_dir / f\"{filename_base}_plots\" # Keep if PNGs might be re-enabled\n",
    "\n",
    "logger.info(f\"Excel report will be saved to: {excel_filename}\")\n",
    "logger.info(f\"PDF report will be saved to: {pdf_filename}\")\n",
    "# logger.info(f\"Individual plots (if generated) will be saved to: {png_dir}\")\n",
    "\n",
    "# --- Other Parameters ---\n",
    "use_dummy_data = ANALYSIS_PARAMS.get('use_dummy', False)\n",
    "filter_qual_types = ANALYSIS_PARAMS.get('filter_qual_types', False)\n",
    "qual_types_to_keep = config.get('qualification_types', []) if filter_qual_types else None\n",
    "\n",
    "logger.info(f\"Use dummy data: {use_dummy_data}\")\n",
    "logger.info(f\"Filter qualification types: {filter_qual_types}\")\n",
    "if filter_qual_types:\n",
    "    logger.info(f\"Qualification types to keep: {qual_types_to_keep}\")\n",
    "\n",
    "logger.info(\"Initial setup parameters resolved.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d821da6",
   "metadata": {},
   "source": [
    "#### 1a - Load Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d31927",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 1a: Loading Raw Data ---\")\n",
    "\n",
    "try:\n",
    "\n",
    "    df_raw = load_data(file_path=data_file, use_dummy=use_dummy)\n",
    "    logger.info(f\"Loaded {len(df_raw)} rows of raw data.\")\n",
    "\n",
    "    # Display sample and info in notebook\n",
    "    print(\"\\n--- Raw Data Sample (First 5 Rows) ---\")\n",
    "    display(df_raw.head())\n",
    "    print(\"\\n--- Raw Data Info ---\")\n",
    "    # Use buffer to capture info output for cleaner display if needed, or print directly\n",
    "    # import io\n",
    "    # buffer = io.StringIO()\n",
    "    # df_raw.info(buf=buffer, verbose=True, show_counts=True)\n",
    "    # print(buffer.getvalue())\n",
    "    df_raw.info(verbose=True, show_counts=True) # Direct print often works fine\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "     logger.error(f\"Data File Error: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during raw data loading: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"Raw data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9dfe63",
   "metadata": {},
   "source": [
    "#### 1b - Extract Data Update Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ee6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 1b: Extracting Data Update Date ---\")\n",
    "\n",
    "# Initialize the variable\n",
    "data_update_date_str = 'N/A' # Or use the default from the function\n",
    "\n",
    "try:\n",
    "    # Ensure df_raw and config exist from previous steps\n",
    "    # Using globals() or locals() is generally preferred for checking existence in notebooks\n",
    "    if 'df_raw' not in globals() or df_raw is None:\n",
    "        logger.warning(\"Raw data (df_raw) is not available. Skipping date extraction.\")\n",
    "        # Set to default if df_raw is missing and you want to continue\n",
    "        data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\")\n",
    "    elif 'config' not in globals() or config is None:\n",
    "        logger.warning(\"Config is not available. Skipping date extraction.\")\n",
    "        # Set to default if config is missing and you want to continue\n",
    "        data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\")\n",
    "    else:\n",
    "        # Call the refactored function\n",
    "        data_update_date_str = extract_data_update_date(df_raw, config)\n",
    "\n",
    "except NameError as e:\n",
    "    logger.error(f\"A required variable (df_raw or config) might not be defined: {e}\")\n",
    "    # Set to default date if needed\n",
    "    data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\")\n",
    "    # Decide if this error should stop execution\n",
    "    # raise # Uncomment to stop execution\n",
    "except Exception as e:\n",
    "     logger.error(f\"An unexpected error occurred calling extract_data_update_date: {e}\", exc_info=True)\n",
    "     # Set to default date if needed\n",
    "     data_update_date_str = datetime.datetime.now().strftime(\"%d.%m.%Y\")\n",
    "     # Decide if this error should stop execution\n",
    "     # raise # Uncomment to stop execution\n",
    "\n",
    "# Print the result regardless of successful extraction or fallback\n",
    "print(f\"Data Update Date: {data_update_date_str}\")\n",
    "logger.info(\"Date extraction step complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963626a9",
   "metadata": {},
   "source": [
    "#### 1c - Clean and Prepare Data (Initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb08ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 1c: Cleaning and Preparing Data (Initial) ---\")\n",
    "df_clean_initial = None # Initialize\n",
    "\n",
    "try:\n",
    "    # Ensure df_raw is available\n",
    "    if df_raw is None:\n",
    "         logger.error(\"Raw data (df_raw) is not available. Cannot perform initial cleaning.\")\n",
    "         raise ValueError(\"df_raw is not defined or loaded.\")\n",
    "\n",
    "    logger.info(\"Applying initial cleaning (merging qualifications, shortening names)...\")\n",
    "    df_clean_initial = clean_and_prepare_data(\n",
    "        df_raw,\n",
    "        institution_names=institution_names_to_match, # From Cell 1\n",
    "        merge_qualifications=True,\n",
    "        shorten_names=True\n",
    "    )\n",
    "    logger.info(f\"Initial cleaning complete. Shape: {df_clean_initial.shape}\")\n",
    "\n",
    "    # Display sample and info\n",
    "    print(\"\\n--- Cleaned Data Sample (Initial) ---\")\n",
    "    display(df_clean_initial.head())\n",
    "    print(\"\\n--- Cleaned Data Info (Initial) ---\")\n",
    "    df_clean_initial.info(verbose=True, show_counts=True)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during initial data cleaning: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"Initial data cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5b20f",
   "metadata": {},
   "source": [
    "#### 1d - Filter by Institution's Offered Qualifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c62737",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 1d: Filtering by Institution's Offered Qualifications ---\")\n",
    "df_prepared = None # Initialize\n",
    "\n",
    "try:\n",
    "    # Ensure df_clean_initial is available\n",
    "    if df_clean_initial is None:\n",
    "         logger.error(\"Cleaned initial data (df_clean_initial) is not available. Cannot filter.\")\n",
    "         raise ValueError(\"df_clean_initial is not defined or loaded.\")\n",
    "\n",
    "    logger.info(f\"Filtering data based on qualifications offered by {institution_short_name}...\")\n",
    "\n",
    "    # Ensure required columns exist before filtering\n",
    "    required_filter_cols = [input_cols['provider'], input_cols['subcontractor'], input_cols['qualification']]\n",
    "    if not all(col in df_clean_initial.columns for col in required_filter_cols):\n",
    "        err_msg = f\"Missing one or more required columns for filtering: {required_filter_cols}. Available: {df_clean_initial.columns.tolist()}\"\n",
    "        logger.error(err_msg)\n",
    "        raise ValueError(err_msg)\n",
    "\n",
    "    # Find qualifications offered by the institution (provider or subcontractor)\n",
    "    institution_mask = (\n",
    "        (df_clean_initial[input_cols['provider']].isin(institution_names_to_match)) |\n",
    "        (df_clean_initial[input_cols['subcontractor']].isin(institution_names_to_match))\n",
    "    )\n",
    "    inst_qualifications = df_clean_initial.loc[institution_mask, input_cols['qualification']].unique()\n",
    "\n",
    "    if len(inst_qualifications) > 0:\n",
    "        # Filter the *entire* dataset to include only rows matching these qualifications\n",
    "        df_prepared = df_clean_initial[df_clean_initial[input_cols['qualification']].isin(inst_qualifications)].copy()\n",
    "        logger.info(f\"Filtered data to {len(inst_qualifications)} qualifications associated with {institution_short_name}. Final shape: {df_prepared.shape}\")\n",
    "    else:\n",
    "        logger.warning(f\"No specific qualifications found associated with {institution_short_name} (variants: {institution_variants}). Using data before this filtering step.\")\n",
    "        df_prepared = df_clean_initial.copy() # Use the data before this specific filtering step\n",
    "\n",
    "    # Display final prepared data sample and info\n",
    "    print(\"\\n--- Prepared Data Sample (Final for Analysis) ---\")\n",
    "    display(df_prepared.head())\n",
    "    print(\"\\n--- Prepared Data Info (Final for Analysis) ---\")\n",
    "    df_prepared.info(verbose=True, show_counts=True)\n",
    "\n",
    "except ValueError as e:\n",
    "     logger.error(f\"Data Error during filtering: {e}\")\n",
    "     raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during qualification filtering: {e}\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"--- Step 1: Data Preparation Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8f13d3",
   "metadata": {},
   "source": [
    "### Step 2: Perform Market Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Step 2a - Apply Optional Qualification Type Filtering\n",
    "logger.info(\"--- Step 2a: Applying Optional Qualification Type Filtering ---\")\n",
    "\n",
    "df_analysis_input = None # Initialize\n",
    "\n",
    "try:\n",
    "    # Ensure data from Step 1 is available\n",
    "    if 'df_prepared' not in globals() or df_prepared is None or df_prepared.empty:\n",
    "        logger.warning(\"Prepared data (df_prepared) is missing or empty. Skipping further analysis steps.\")\n",
    "        # Set df_analysis_input to None or an empty DataFrame to prevent downstream errors\n",
    "        df_analysis_input = pd.DataFrame() # Or None, depending on how downstream cells handle it\n",
    "        # Optionally, raise an error if analysis cannot proceed:\n",
    "        # raise ValueError(\"Prepared data is required for analysis but is missing or empty.\")\n",
    "    elif 'filter_qual_types_flag' not in globals():\n",
    "        logger.warning(\"filter_qual_types_flag not defined. Assuming False.\")\n",
    "        filter_qual_types_flag = False # Set a default\n",
    "        df_analysis_input = df_prepared.copy() # Use prepared data directly\n",
    "    elif 'config' not in globals():\n",
    "         logger.error(\"Config object not found. Cannot apply filtering.\")\n",
    "         raise NameError(\"Config object not found.\")\n",
    "    else:\n",
    "        # Apply filtering logic\n",
    "        if filter_qual_types_flag:\n",
    "            # Ensure 'qualification_types' exists in config, provide default if not\n",
    "            qual_types = config.get('qualification_types', ['Ammattitutkinnot', 'Erikoisammattitutkinnot'])\n",
    "            logger.info(f\"Applying qualification type filter for: {qual_types}\")\n",
    "            # Ensure 'tutkintotyyppi' column exists in df_prepared\n",
    "            if 'tutkintotyyppi' not in df_prepared.columns:\n",
    "                 logger.error(\"Column 'tutkintotyyppi' not found in df_prepared. Cannot apply filter.\")\n",
    "                 raise KeyError(\"Column 'tutkintotyyppi' missing for filtering.\")\n",
    "            df_analysis_input = df_prepared[df_prepared['tutkintotyyppi'].isin(qual_types)].copy()\n",
    "            logger.info(f\"Shape before type filter: {df_prepared.shape}, after: {df_analysis_input.shape}\")\n",
    "        else:\n",
    "            logger.info(\"Skipping qualification type filtering.\")\n",
    "            df_analysis_input = df_prepared.copy() # Use the prepared data directly\n",
    "\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Configuration or Data Error during filtering: {e}\")\n",
    "    raise # Stop execution\n",
    "except NameError as e:\n",
    "     logger.error(f\"Variable Error during filtering setup: {e}\")\n",
    "     raise # Stop execution\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during optional filtering: {e}\", exc_info=True)\n",
    "    raise # Stop execution\n",
    "\n",
    "logger.info(\"Step 2a filtering complete.\")\n",
    "# Display shape or head if desired\n",
    "if df_analysis_input is not None:\n",
    "    print(f\"Shape of data for analysis: {df_analysis_input.shape}\")\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37d7cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config check\n",
    "# print(\"--- Checking config before MarketAnalyzer init ---\")\n",
    "# print(f\"Config type: {type(config)}\")\n",
    "# if isinstance(config, dict):\n",
    "#     print(f\"config['columns']['output'] keys: {config.get('columns', {}).get('output', {}).keys()}\")\n",
    "#     print(f\"market_rank exists: {'market_rank' in config.get('columns', {}).get('output', {})}\")\n",
    "#     print(f\"market_share_growth exists: {'market_share_growth' in config.get('columns', {}).get('output', {})}\")\n",
    "# else:\n",
    "#     print(\"Config object is not a dictionary!\")\n",
    "# print(\"--- End config check ---\")\n",
    "\n",
    "# logger.info(\"Initializing MarketAnalyzer...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e36d86",
   "metadata": {},
   "source": [
    "#### Step 2b - Initialize and Run Market Analyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ed272",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 2b: Initializing and Running Market Analyzer ---\")\n",
    "\n",
    "analyzer = None\n",
    "analysis_results = {}\n",
    "\n",
    "try:\n",
    "    # Ensure necessary inputs are available\n",
    "    if df_analysis_input is None or df_analysis_input.empty:\n",
    "        logger.warning(\"Input data (df_analysis_input) is missing or empty. Skipping Market Analysis.\")\n",
    "        # Keep analysis_results empty and analyzer None\n",
    "    elif 'config' not in globals() or not config:\n",
    "         logger.error(\"Config object not found or empty. Cannot initialize Analyzer.\")\n",
    "         raise NameError(\"Config object not found or empty.\")\n",
    "    elif 'institution_variants' not in globals() or 'institution_short_name' not in globals():\n",
    "         logger.error(\"Institution details (variants or short_name) not found. Cannot initialize Analyzer.\")\n",
    "         raise NameError(\"Institution details not found.\")\n",
    "    else:\n",
    "        # --- FIX for KeyError: Ensure the required output column name exists in config ---\n",
    "        # Define a default name if missing from config.yaml\n",
    "        default_share_growth_col_name = 'Market Share Growth (%)'\n",
    "        if 'columns' not in config: config['columns'] = {}\n",
    "        if 'output' not in config['columns']: config['columns']['output'] = {}\n",
    "        if 'market_share_growth' not in config['columns']['output']:\n",
    "            logger.warning(f\"Config missing ['columns']['output']['market_share_growth']. Using default: '{default_share_growth_col_name}'\")\n",
    "            config['columns']['output']['market_share_growth'] = default_share_growth_col_name\n",
    "        # -----------------------------------------------------------------------------\n",
    "\n",
    "        logger.info(\"Initializing MarketAnalyzer...\")\n",
    "        analyzer = MarketAnalyzer(df_analysis_input, cfg=config)\n",
    "        # Set institution details on the analyzer instance\n",
    "        analyzer.institution_names = institution_names_to_match\n",
    "        analyzer.institution_short_name = institution_short_name\n",
    "        logger.info(f\"Analyzer configured for: {institution_short_name} ({institution_variants})\")\n",
    "\n",
    "        logger.info(\"Running analysis...\")\n",
    "        analysis_results = analyzer.analyze() # This returns a dict of DataFrames\n",
    "        logger.info(f\"Analysis complete. Results keys: {list(analysis_results.keys())}\")\n",
    "\n",
    "        logger.info(\"--- Step 2b: Market Analysis Complete ---\")\n",
    "\n",
    "# Handle specific errors if needed, e.g., KeyError from config\n",
    "except NameError as e:\n",
    "     logger.error(f\"Initialization Error: Required variable not found: {e}\")\n",
    "     raise # Stop execution\n",
    "except KeyError as e:\n",
    "     logger.error(f\"Configuration Error during analysis: Missing key {e}\")\n",
    "     raise # Stop execution\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during market analysis: {e}\", exc_info=True)\n",
    "    analysis_results = {} # Ensure it exists but is empty on error\n",
    "    analyzer = None # Ensure it's None on error\n",
    "    raise # Re-raise the exception instead of sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e20281",
   "metadata": {},
   "source": [
    "#### Step 2c - Display Analysis Results Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1cff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 2c: Displaying Analysis Results Samples ---\")\n",
    "\n",
    "try:\n",
    "    if 'analysis_results' not in globals() or not analysis_results:\n",
    "         logger.warning(\"Analysis results are not available or empty. Nothing to display.\")\n",
    "    else:\n",
    "        # Print head of key results DataFrames for notebook-like inspection\n",
    "        print(\"\\n--- Analysis Results Samples ---\")\n",
    "\n",
    "        for key, df_result in analysis_results.items():\n",
    "            print(f\"\\n--- Result: {key} ---\")\n",
    "            if isinstance(df_result, pd.DataFrame):\n",
    "                if not df_result.empty:\n",
    "                    print(f\"(Top 5 rows of {df_result.shape[0]} total)\")\n",
    "                    display(df_result.head())\n",
    "                else:\n",
    "                     print(\"(DataFrame is empty)\")\n",
    "            else:\n",
    "                # Handle non-DataFrame results if any (e.g., scalars, lists)\n",
    "                 print(f\"(Type: {type(df_result)})\")\n",
    "                 print(df_result)\n",
    "\n",
    "except NameError:\n",
    "    logger.error(\"Variable 'analysis_results' not defined. Cannot display results.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred displaying analysis results: {e}\", exc_info=True)\n",
    "\n",
    "logger.info(\"Step 2c display complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3c76c",
   "metadata": {},
   "source": [
    "### Step 3: Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c2fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3: Generating Visualizations ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af83c8e",
   "metadata": {},
   "source": [
    "Define directory for saving plots --> Will use main output dir for PDF\n",
    "plots_output_dir = None \n",
    "figures = {} # Initialize dictionary to store figure paths --> Not needed for PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae3f71",
   "metadata": {},
   "source": [
    "#### Step 3a - Visualization Setup & Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa730014",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3a: Initializing Visualizer and Preparing Plot Data ---\")\n",
    "\n",
    "pdf_report_path = None # Store path to the generated PDF\n",
    "visualizer = None      # Define visualizer in outer scope for subsequent cells\n",
    "active_qualifications = [] # Define active_qualifications in outer scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get Config Column Names & Other Parameters ---\n",
    "logger.info(\"Gathering parameters for plotting...\")\n",
    "cols_out = config.get('columns', {}).get('output', {})\n",
    "# Ensure necessary keys exist in cols_out, provide defaults if needed (should be less necessary now)\n",
    "year_col = cols_out.get('year', 'Vuosi')\n",
    "qual_col = cols_out.get('qualification', 'Tutkinto')\n",
    "provider_col = cols_out.get('provider', 'Oppilaitos')\n",
    "provider_amount_col = cols_out.get('provider_amount', 'NOM järjestäjänä')\n",
    "subcontractor_amount_col = cols_out.get('subcontractor_amount', 'NOM hankintana')\n",
    "total_volume_col = cols_out.get('total_volume', 'NOM yhteensä')\n",
    "market_total_col = cols_out.get('market_total', 'Markkina yhteensä')\n",
    "market_share_col = cols_out.get('market_share', 'Markkinaosuus (%)')\n",
    "market_rank_col = cols_out.get('market_rank', 'Sijoitus markkinaosuuden mukaan') # Include rank if needed later\n",
    "market_share_growth_col = cols_out.get('market_share_growth', 'Markkinaosuuden kasvu (%)') # Include growth if needed later\n",
    "\n",
    "# Default/Fixed names for columns generated by MarketAnalyzer internally\n",
    "bcg_growth_col = 'Market Growth (%)'\n",
    "bcg_share_col = 'Relative Market Share'\n",
    "bcg_size_col = 'Institution Volume'\n",
    "count_provider_col = 'Unique_Providers_Count'\n",
    "count_subcontractor_col = 'Unique_Subcontractors_Count'\n",
    "\n",
    "# Get info from analyzer\n",
    "inst_short_name = analyzer.institution_short_name\n",
    "inst_names = analyzer.institution_names\n",
    "min_year = analyzer.min_year\n",
    "max_year = analyzer.max_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07d3a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base caption and reference year\n",
    "if \"TEXT_CONSTANTS\" not in locals() or \"COLOR_PALETTES\" not in locals():\n",
    "    # Import if not available globally (e.g., if imports moved from main cell)\n",
    "    from src.vipunen.visualization.education_visualizer import TEXT_CONSTANTS, COLOR_PALETTES\n",
    "    logger.info(\"Imported TEXT_CONSTANTS and COLOR_PALETTES locally for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb77e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Ensure required variables from previous steps are available\n",
    "    if 'analysis_results' not in locals() or not analysis_results:\n",
    "        raise NameError(\"Analysis results (analysis_results) not found or empty.\")\n",
    "    if 'analyzer' not in locals() or analyzer is None:\n",
    "        raise NameError(\"Analyzer instance (analyzer) not found.\")\n",
    "    if 'config' not in locals() or not config:\n",
    "         raise NameError(\"Config object not found or empty.\")\n",
    "    if 'institution_short_name' not in locals() or not institution_short_name:\n",
    "         raise NameError(\"Institution short name not found.\")\n",
    "    if 'institution_variants' not in locals(): # Check for variants used in filtering\n",
    "         raise NameError(\"Institution variants list not found.\")\n",
    "    if 'data_update_date_str' not in locals():\n",
    "         raise NameError(\"Data update date string not found.\")\n",
    "\n",
    "    # --- Determine Output Directory for PDF --- \n",
    "    # Reuse the output_base_dir calculated in the main setup cell\n",
    "    if 'output_base_dir' not in globals() or not isinstance(output_base_dir, Path):\n",
    "        logger.error(\"Base output directory (output_base_dir) not found or not a Path object. Cannot determine PDF path.\")\n",
    "        raise NameError(\"output_base_dir is not defined or is not a Path\")\n",
    "    \n",
    "    report_dir_name = f\"education_market_{institution_short_name.lower()}\" \n",
    "    report_output_dir = output_base_dir / report_dir_name\n",
    "    report_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Log relative path using the root_path Path object\n",
    "    try:\n",
    "        relative_log_path = report_output_dir.relative_to(root_path)\n",
    "    except ValueError:\n",
    "        logger.warning(f\"Could not determine relative path for logging PDF directory {report_output_dir}. Logging absolute path instead.\")\n",
    "        relative_log_path = report_output_dir # Fallback to absolute path if relative fails\n",
    "    logger.info(f\"PDF report will be saved in subdirectory: {relative_log_path}\") \n",
    "\n",
    "    # --- Initialize Visualizer ---\n",
    "    logger.info(\"Initializing EducationVisualizer for PDF output...\")\n",
    "    visualizer = EducationVisualizer(\n",
    "        style=\"default\",\n",
    "        output_dir=report_output_dir,\n",
    "        output_format='pdf',\n",
    "        institution_short_name=institution_short_name,\n",
    "        include_timestamp=ANALYSIS_PARAMS.get('include_timestamp', True)\n",
    "    )\n",
    "    visualizer.data_update_date = data_update_date_str\n",
    "    logger.info(f\"Visualizer initialized. PDF will be saved to: {visualizer.pdf_path}\") # Log expected path\n",
    "\n",
    "    base_caption = TEXT_CONSTANTS[\"data_source\"].format(date=data_update_date_str)\n",
    "    last_full_year = max_year - 1 if max_year and min_year and max_year > min_year else max_year\n",
    "    plot_reference_year = last_full_year if last_full_year else max_year\n",
    "    logger.info(f\"Plotting parameters: Ref Year={plot_reference_year}, Caption Date={data_update_date_str}\")\n",
    "\n",
    "    # --- Determine Active Qualifications ---\n",
    "    logger.info(\"Determining active qualifications...\")\n",
    "    detailed_df = analysis_results.get('detailed_providers_market')\n",
    "    if detailed_df is not None and not detailed_df.empty and max_year is not None and min_year is not None:\n",
    "        # Logic to find active qualifications (copied from original cell)\n",
    "        years_to_check_activity = [last_full_year]\n",
    "        prev_full_year = last_full_year - 1 if last_full_year > min_year else None\n",
    "        if prev_full_year: years_to_check_activity.append(prev_full_year)\n",
    "\n",
    "        # Ensure required columns exist in detailed_df\n",
    "        required_active_cols = [provider_col, year_col, total_volume_col, qual_col, market_share_col]\n",
    "        if not all(c in detailed_df.columns for c in required_active_cols):\n",
    "             missing_cols = [c for c in required_active_cols if c not in detailed_df.columns]\n",
    "             raise KeyError(f\"Missing required columns in detailed_providers_market for active qual calc: {missing_cols}\")\n",
    "\n",
    "        inst_recent_df = detailed_df[(detailed_df[provider_col].isin(inst_names)) & (detailed_df[year_col].isin(years_to_check_activity))].copy()\n",
    "        analysis_config = config.get('analysis', {})\n",
    "        min_volume_sum_threshold = analysis_config.get('active_qualification_min_volume_sum', 3)\n",
    "\n",
    "        # Calculate volume sum\n",
    "        volume_grouped = inst_recent_df.groupby(qual_col)[total_volume_col].sum()\n",
    "        quals_with_recent_volume = set(volume_grouped[volume_grouped >= min_volume_sum_threshold].index)\n",
    "\n",
    "        # Calculate those with 100% share in both recent years\n",
    "        quals_with_100_share_both_years = set()\n",
    "        if prev_full_year is not None:\n",
    "            inst_recent_df[market_share_col] = pd.to_numeric(inst_recent_df[market_share_col], errors='coerce')\n",
    "            inst_100_share_df = inst_recent_df[inst_recent_df[market_share_col].round(2) == 100.0]\n",
    "            share_counts = inst_100_share_df.groupby(qual_col)[year_col].nunique()\n",
    "            quals_with_100_share_both_years = set(share_counts[share_counts == 2].index)\n",
    "\n",
    "        # Final active set\n",
    "        active_qualifications_set = quals_with_recent_volume - quals_with_100_share_both_years\n",
    "        active_qualifications = sorted(list(active_qualifications_set))\n",
    "        logger.info(f\"Determined {len(active_qualifications)} active qualifications for plots.\")\n",
    "    else:\n",
    "        logger.warning(\"Could not determine active qualifications (detailed data missing or empty). Plots might be skipped or use fallback.\")\n",
    "        # Fallback: use all qualifications the institution ever offered\n",
    "        if detailed_df is not None and not detailed_df.empty and provider_col in detailed_df.columns and qual_col in detailed_df.columns:\n",
    "             active_qualifications = sorted(list(detailed_df[detailed_df[provider_col].isin(inst_names)][qual_col].unique()))\n",
    "             logger.info(f\"Using fallback: {len(active_qualifications)} qualifications institution ever offered.\")\n",
    "\n",
    "except NameError as e:\n",
    "    logger.error(f\"Setup Error: Required variable not found: {e}. Cannot proceed with plotting.\")\n",
    "    # Optionally set a flag to skip subsequent plot cells\n",
    "    _skip_plots = True\n",
    "except KeyError as e:\n",
    "    logger.error(f\"Setup Error: Missing expected key: {e}. Cannot proceed with plotting.\")\n",
    "    _skip_plots = True\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during visualization setup: {e}\", exc_info=True)\n",
    "    _skip_plots = True # Skip plots on generic error too\n",
    "else:\n",
    "     _skip_plots = False # Ensure flag is False if setup succeeds\n",
    "\n",
    "logger.info(\"--- Step 3a: Visualization Setup Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b2eec",
   "metadata": {},
   "source": [
    "#### Step 3b - Plot 1: Stacked Area (Total Volumes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Step 3b - Plot 1: Stacked Area (Total Volumes)\n",
    "logger.info(\"--- Step 3b: Generating Plot 1 (Total Volumes Area Chart) ---\")\n",
    "\n",
    "# Check if setup failed or if visualizer is not ready\n",
    "if '_skip_plots' in globals() and _skip_plots or 'visualizer' not in globals() or visualizer is None:\n",
    "    logger.warning(\"Skipping Plot 1 due to setup issues or missing visualizer.\")\n",
    "else:\n",
    "    fig = None # Ensure fig is defined for finally block\n",
    "    try:\n",
    "        total_volumes_df = analysis_results.get('total_volumes')\n",
    "        required_cols = [year_col, provider_amount_col, subcontractor_amount_col]\n",
    "\n",
    "        if total_volumes_df is None or total_volumes_df.empty:\n",
    "            logger.warning(\"Skipping Total Volumes plot: Data ('total_volumes') not available or empty.\")\n",
    "        elif not all(c in total_volumes_df.columns for c in required_cols):\n",
    "             missing = [c for c in required_cols if c not in total_volumes_df.columns]\n",
    "             logger.warning(f\"Skipping Total Volumes plot: Missing columns {missing}.\")\n",
    "        else:\n",
    "            logger.info(\"Data found. Generating Total Volumes Area Chart...\")\n",
    "            plot_df_roles = total_volumes_df.rename(columns={\n",
    "                provider_amount_col: 'järjestäjänä',\n",
    "                subcontractor_amount_col: 'hankintana'\n",
    "            })\n",
    "            if 'järjestäjänä' in plot_df_roles.columns and 'hankintana' in plot_df_roles.columns:\n",
    "                fig, _ = visualizer.create_area_chart(\n",
    "                    data=plot_df_roles, x_col=year_col, y_cols=['järjestäjänä', 'hankintana'],\n",
    "                    colors=[COLOR_PALETTES[\"roles\"][\"järjestäjänä\"], COLOR_PALETTES[\"roles\"][\"hankintana\"]],\n",
    "                    labels=['järjestäjänä', 'hankintana'], title=f\"{inst_short_name} netto-opiskelijamäärä vuosina {min_year}-{max_year}\",\n",
    "                    caption=base_caption, stacked=True\n",
    "                )\n",
    "                visualizer.save_visualization(fig, f\"{inst_short_name}_total_volumes_area\") # Save to PDF\n",
    "                logger.info(\"Plot 1 generated and saved to PDF.\")\n",
    "                display(fig) # <<< ADD THIS LINE TO SHOW INLINE\n",
    "            else:\n",
    "                logger.warning(\"Failed to rename columns for Total Volumes plot.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate Plot 1 (Total Volumes): {e}\", exc_info=True)\n",
    "    finally:\n",
    "         # Ensure figure is closed AFTER displaying\n",
    "        if fig is not None and plt.fignum_exists(fig.number):\n",
    "            plt.close(fig)\n",
    "            logger.debug(\"Closed Plot 1 figure.\")\n",
    "\n",
    "logger.info(\"--- Step 3b: Plot 1 Generation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e116ff4",
   "metadata": {},
   "source": [
    "#### Step 3c - Plot 2: Line Charts (Market Share Evolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e34861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Step 3c - Plot 2: Line Charts (Market Share Evolution)\n",
    "logger.info(\"--- Step 3c: Generating Plot 2 (Market Share Line Charts) ---\")\n",
    "\n",
    "# Check if setup failed or if visualizer is not ready\n",
    "if '_skip_plots' in globals() and _skip_plots or 'visualizer' not in globals() or visualizer is None:\n",
    "    logger.warning(\"Skipping Plot 2 due to setup issues or missing visualizer.\")\n",
    "elif 'active_qualifications' not in globals() or not active_qualifications:\n",
    "     logger.warning(\"Skipping Plot 2: No active qualifications determined.\")\n",
    "else:\n",
    "    detailed_df = analysis_results.get('detailed_providers_market')\n",
    "    required_cols = [qual_col, year_col, provider_col, market_share_col]\n",
    "\n",
    "    if detailed_df is None or detailed_df.empty:\n",
    "        logger.warning(\"Skipping Market Share Line Charts: Data ('detailed_providers_market') missing or empty.\")\n",
    "    elif not all(c in detailed_df.columns for c in required_cols):\n",
    "        missing = [c for c in required_cols if c not in detailed_df.columns]\n",
    "        logger.warning(f\"Skipping Market Share Line Charts: Missing columns {missing}.\")\n",
    "    else:\n",
    "        logger.info(f\"Generating Market Share Line Charts for {len(active_qualifications)} active qualifications...\")\n",
    "        processed_count = 0\n",
    "        for qual in active_qualifications:\n",
    "            fig = None # Define fig inside loop for finally block\n",
    "            try:\n",
    "                qual_df = detailed_df[detailed_df[qual_col] == qual]\n",
    "                if qual_df.empty:\n",
    "                    logger.debug(f\"Skipping line chart for {qual}: No data.\")\n",
    "                    continue\n",
    "\n",
    "                latest_qual_providers = qual_df[qual_df[year_col] == plot_reference_year]\n",
    "                # Use nlargest, handling cases where there are fewer than 6 providers\n",
    "                top_m_providers = latest_qual_providers.nlargest(6, market_share_col)[provider_col].unique().tolist()\n",
    "\n",
    "                if not top_m_providers:\n",
    "                     logger.debug(f\"Skipping line chart for {qual}: No providers found for top M.\")\n",
    "                     continue\n",
    "\n",
    "                # Pivot data for plotting\n",
    "                plot_data = qual_df[qual_df[provider_col].isin(top_m_providers)].pivot(index=year_col, columns=provider_col, values=market_share_col)\n",
    "\n",
    "                if not plot_data.empty:\n",
    "                    plot_data.index.name = year_col # Ensure index name for x_col reference\n",
    "                    # Ensure y_cols exist in the pivoted data\n",
    "                    valid_y_cols = [col for col in top_m_providers if col in plot_data.columns]\n",
    "                    \n",
    "                    if not valid_y_cols:\n",
    "                        logger.debug(f\"Skipping line chart for {qual}: No valid columns after pivot.\")\n",
    "                        continue\n",
    "\n",
    "                    # Create filename part robustly\n",
    "                    qual_filename_part = \"\".join(c if c.isalnum() else \"_\" for c in qual).lower()[:50]\n",
    "\n",
    "                    fig, _ = visualizer.create_line_chart(\n",
    "                        data=plot_data, x_col=plot_data.index, y_cols=valid_y_cols,\n",
    "                        colors=COLOR_PALETTES[\"main\"], labels=valid_y_cols, # Use valid cols here too\n",
    "                        title=f\"{qual}: Markkinaosuus (%)\", caption=base_caption, markers=True\n",
    "                    )\n",
    "                    visualizer.save_visualization(fig, f\"{inst_short_name}_{qual_filename_part}_market_share_lines\") # Save to PDF\n",
    "                    display(fig) # <<< ADD THIS LINE TO SHOW INLINE\n",
    "                    processed_count += 1\n",
    "                else:\n",
    "                    logger.debug(f\"Skipping line chart for {qual}: Pivoted data is empty.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to generate Market Share line plot for '{qual}': {e}\", exc_info=True)\n",
    "            finally:\n",
    "                # Close figure AFTER potential display\n",
    "                if fig is not None and plt.fignum_exists(fig.number):\n",
    "                    plt.close(fig)\n",
    "                    logger.debug(f\"Closed figure for {qual}.\")\n",
    "        logger.info(f\"Plot 2 generation attempted. Successfully processed {processed_count}/{len(active_qualifications)} charts.\")\n",
    "\n",
    "logger.info(\"--- Step 3c: Plot 2 Generation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f11b3f4",
   "metadata": {},
   "source": [
    "#### Step 3d - Plot 3: Heatmap (Institution's Market Share)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e71696",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3d: Generating Plot 3 (Market Share Heatmap) ---\")\n",
    "\n",
    "# Check if setup failed or if visualizer is not ready\n",
    "if '_skip_plots' in globals() and _skip_plots or 'visualizer' not in globals() or visualizer is None:\n",
    "    logger.warning(\"Skipping Plot 3 due to setup issues or missing visualizer.\")\n",
    "elif 'active_qualifications' not in globals() or not active_qualifications:\n",
    "     logger.warning(\"Skipping Plot 3: No active qualifications determined.\")\n",
    "else:\n",
    "    fig = None # Define fig for finally block\n",
    "    try:\n",
    "        detailed_df = analysis_results.get('detailed_providers_market')\n",
    "        required_cols = [provider_col, year_col, qual_col, market_share_col] # Basic cols needed\n",
    "\n",
    "        if detailed_df is None or detailed_df.empty:\n",
    "            logger.warning(\"Skipping Heatmap: Data ('detailed_providers_market') missing or empty.\")\n",
    "        elif not all(c in detailed_df.columns for c in required_cols):\n",
    "             missing = [c for c in required_cols if c not in detailed_df.columns]\n",
    "             logger.warning(f\"Skipping Heatmap: Missing columns {missing}.\")\n",
    "        else:\n",
    "            logger.info(\"Data found. Generating Institution Market Share Heatmap...\")\n",
    "            # Filter for institution, handling multiple variants by aggregation if needed\n",
    "            inst_share_df_raw = detailed_df[detailed_df[provider_col].isin(inst_names)].copy()\n",
    "            inst_share_df = inst_share_df_raw # Default if only one variant\n",
    "\n",
    "            if len(inst_names) > 1 and not inst_share_df_raw.empty:\n",
    "                logger.info(\"Aggregating data for multiple institution variants for heatmap...\")\n",
    "                # Aggregation logic (copied, assumes columns exist based on earlier checks)\n",
    "                agg_logic = {\n",
    "                    provider_amount_col: 'sum', subcontractor_amount_col: 'sum',\n",
    "                    total_volume_col: 'sum', market_total_col: 'first',\n",
    "                    market_share_col: 'first', # Placeholder\n",
    "                }\n",
    "                # Filter agg_logic based on actual columns present (handle optional cols)\n",
    "                agg_logic = {k: v for k, v in agg_logic.items() if k in inst_share_df_raw.columns}\n",
    "\n",
    "                if agg_logic and total_volume_col in agg_logic and market_total_col in agg_logic :\n",
    "                    inst_share_df_agg = inst_share_df_raw.groupby([year_col, qual_col], as_index=False).agg(agg_logic)\n",
    "                    # Recalculate market share\n",
    "                    valid_market_total = inst_share_df_agg[market_total_col] > 0\n",
    "                    inst_share_df_agg[market_share_col] = 0.0\n",
    "                    inst_share_df_agg.loc[valid_market_total, market_share_col] = (inst_share_df_agg.loc[valid_market_total, total_volume_col] / inst_share_df_agg.loc[valid_market_total, market_total_col] * 100)\n",
    "                    inst_share_df = inst_share_df_agg\n",
    "                else:\n",
    "                     logger.warning(\"Aggregation skipped for heatmap: Missing required columns for aggregation (e.g., volume/market totals).\")\n",
    "                     # Decide: use raw (might be misleading) or skip? Let's try using raw.\n",
    "                     # inst_share_df = inst_share_df_raw # Already set\n",
    "\n",
    "            # Filter for active qualifications and pivot\n",
    "            if not inst_share_df.empty:\n",
    "                inst_share_df_active = inst_share_df[inst_share_df[qual_col].isin(active_qualifications)]\n",
    "            else:\n",
    "                 inst_share_df_active = pd.DataFrame() # Ensure empty df if no inst data\n",
    "\n",
    "            if not inst_share_df_active.empty and market_share_col in inst_share_df_active.columns:\n",
    "                heatmap_data = inst_share_df_active.pivot_table(index=qual_col, columns=year_col, values=market_share_col)\n",
    "                heatmap_data = heatmap_data.sort_index() # Sort rows alphabetically by qualification\n",
    "\n",
    "                if not heatmap_data.empty:\n",
    "                    fig, _ = visualizer.create_heatmap(\n",
    "                        data=heatmap_data, title=f\"{inst_short_name} markkinaosuus (%) aktiivisissa tutkinnoissa\",\n",
    "                        caption=base_caption, cmap=\"Greens\", annot=True, fmt=\".1f\"\n",
    "                    )\n",
    "                    visualizer.save_visualization(fig, f\"{inst_short_name}_market_share_heatmap_active\")\n",
    "                    logger.info(\"Plot 3 generated and saved to PDF.\")\n",
    "                    display(fig) \n",
    "                else:\n",
    "                     logger.warning(f\"Skipping Heatmap: Pivoted data is empty for active qualifications.\")\n",
    "            else:\n",
    "                 logger.warning(f\"Skipping Heatmap: No data for {inst_short_name} in active qualifications or missing market share column.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate Plot 3 (Heatmap): {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if fig is not None and plt.fignum_exists(fig.number):\n",
    "            plt.close(fig)\n",
    "            logger.debug(\"Closed Plot 3 figure.\")\n",
    "\n",
    "logger.info(\"--- Step 3d: Plot 3 Generation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aabb23",
   "metadata": {},
   "source": [
    "#### Step 3e - Plot 4: BCG Growth-Share Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94fc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3e: Generating Plot 4 (BCG Matrix) ---\")\n",
    "\n",
    "# Check if setup failed or if visualizer is not ready\n",
    "if _skip_plots or visualizer is None:\n",
    "    logger.warning(\"Skipping Plot 4 due to setup issues or missing visualizer.\")\n",
    "else:\n",
    "    fig = None # Define fig for finally block\n",
    "    try:\n",
    "        bcg_data_df = analysis_results.get('bcg_data')\n",
    "        # Columns defined in setup cell: qual_col, bcg_growth_col, bcg_share_col, bcg_size_col\n",
    "        required_cols = [bcg_growth_col, bcg_share_col, bcg_size_col] # Label col checked separately\n",
    "\n",
    "        if bcg_data_df is None or bcg_data_df.empty:\n",
    "             logger.warning(\"Skipping BCG Matrix: Data ('bcg_data') not available or empty.\")\n",
    "        elif not all(c in bcg_data_df.columns for c in required_cols):\n",
    "             missing = [c for c in required_cols if c not in bcg_data_df.columns]\n",
    "             logger.warning(f\"Skipping BCG Matrix: Missing required data columns {missing}. Found: {bcg_data_df.columns.tolist()}\")\n",
    "        else:\n",
    "            # Find the actual qualification column name used in the bcg_data df\n",
    "            actual_qual_col = None\n",
    "            if qual_col in bcg_data_df.columns: # Check if config output name is used\n",
    "                 actual_qual_col = qual_col\n",
    "            elif 'Qualification' in bcg_data_df.columns: # Check for hardcoded alternative\n",
    "                 actual_qual_col = 'Qualification'\n",
    "            # Add more checks if other names are possible\n",
    "\n",
    "            if actual_qual_col is None:\n",
    "                 logger.warning(f\"Skipping BCG Matrix: Could not find qualification/label column (checked '{qual_col}', 'Qualification'). Found: {bcg_data_df.columns.tolist()}\")\n",
    "            else:\n",
    "                logger.info(\"Data found. Generating BCG Growth-Share Matrix...\")\n",
    "                plot_title = f\"{inst_short_name}: Tutkintojen kasvu vs. markkinaosuus ({plot_reference_year})\"\n",
    "                bcg_caption = base_caption + f\" Kuplan koko = {inst_short_name} volyymi ({plot_reference_year}). Suhteellinen markkinaosuus = {inst_short_name} osuus / Suurimman kilpailijan osuus.\"\n",
    "\n",
    "                fig, _ = visualizer.create_bcg_matrix(\n",
    "                    data=bcg_data_df, growth_col=bcg_growth_col, share_col=bcg_share_col,\n",
    "                    size_col=bcg_size_col, label_col=actual_qual_col, # Use the found column\n",
    "                    title=plot_title, caption=bcg_caption\n",
    "                )\n",
    "                visualizer.save_visualization(fig, f\"{inst_short_name}_bcg_matrix\")\n",
    "                logger.info(\"Plot 4 generated and saved to PDF.\")\n",
    "                display(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate Plot 4 (BCG Matrix): {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if fig is not None and plt.fignum_exists(fig.number):\n",
    "            plt.close(fig)\n",
    "            logger.debug(\"Closed Plot 4 figure.\")\n",
    "\n",
    "logger.info(\"--- Step 3e: Plot 4 Generation Complete ---\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf396ec5",
   "metadata": {},
   "source": [
    "#### Step 3f - Plot 5: Combined Volume / Provider Count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae93de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3f: Generating Plot 5 (Volume / Provider Count) ---\")\n",
    "\n",
    "# Check if setup failed or if visualizer is not ready\n",
    "if _skip_plots or visualizer is None:\n",
    "    logger.warning(\"Skipping Plot 5 due to setup issues or missing visualizer.\")\n",
    "else:\n",
    "    fig = None # Define fig for finally block\n",
    "    try:\n",
    "        volume_df = analysis_results.get('total_volumes')\n",
    "        count_df = analysis_results.get('provider_counts_by_year')\n",
    "        # Required columns defined in setup cell\n",
    "        req_vol_cols = [year_col, provider_amount_col, subcontractor_amount_col]\n",
    "        req_count_cols = [year_col, count_provider_col, count_subcontractor_col]\n",
    "\n",
    "        if volume_df is None or volume_df.empty:\n",
    "             logger.warning(\"Skipping Volume/Provider Count plot: Volume data ('total_volumes') missing or empty.\")\n",
    "        elif count_df is None or count_df.empty:\n",
    "             logger.warning(\"Skipping Volume/Provider Count plot: Count data ('provider_counts_by_year') missing or empty.\")\n",
    "        elif not all(c in volume_df.columns for c in req_vol_cols):\n",
    "             missing = [c for c in req_vol_cols if c not in volume_df.columns]\n",
    "             logger.warning(f\"Skipping Volume/Provider Count plot: Missing volume columns {missing}.\")\n",
    "        elif not all(c in count_df.columns for c in req_count_cols):\n",
    "             missing = [c for c in req_count_cols if c not in count_df.columns]\n",
    "             logger.warning(f\"Skipping Volume/Provider Count plot: Missing count columns {missing}.\")\n",
    "        else:\n",
    "            logger.info(\"Data found. Generating Volume / Provider Count Plot...\")\n",
    "            plot_title = f\"{inst_short_name}: Opiskelijamäärät ja kouluttajamarkkina ({min_year}-{max_year})\"\n",
    "            plot_caption = base_caption + f\". Kouluttajamäärä perustuu tutkintoihin, joita {inst_short_name} tarjoaa.\"\n",
    "\n",
    "            fig, _ = visualizer.create_volume_and_provider_count_plot(\n",
    "                volume_data=volume_df, count_data=count_df, title=plot_title,\n",
    "                volume_title=\"Netto-opiskelijamäärä\", count_title=\"Uniikit kouluttajat markkinassa\",\n",
    "                year_col=year_col, vol_provider_col=provider_amount_col, vol_subcontractor_col=subcontractor_amount_col,\n",
    "                count_provider_col=count_provider_col, count_subcontractor_col=count_subcontractor_col,\n",
    "                caption=plot_caption\n",
    "            )\n",
    "            visualizer.save_visualization(fig, f\"{inst_short_name}_volume_provider_counts\")\n",
    "            logger.info(\"Plot 5 generated and saved to PDF.\")\n",
    "            display(fig)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to generate Plot 5 (Volume/Provider Count): {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if fig is not None and plt.fignum_exists(fig.number):\n",
    "            plt.close(fig)\n",
    "            logger.debug(\"Closed Plot 5 figure.\")\n",
    "\n",
    "logger.info(\"--- Step 3f: Plot 5 Generation Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fdf227",
   "metadata": {},
   "source": [
    "#### Step 3g - Finalize PDF Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4813ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 3g: Finalizing PDF Report ---\")\n",
    "\n",
    "# Check if setup failed or if visualizer is not ready\n",
    "if _skip_plots or visualizer is None:\n",
    "    logger.warning(\"Skipping PDF finalization due to setup issues or missing visualizer.\")\n",
    "    pdf_report_path = None # Ensure path is None\n",
    "else:\n",
    "    try:\n",
    "        logger.info(f\"Closing PDF file: {visualizer.pdf_path}...\")\n",
    "        # The close_pdf method implicitly saves the collected pages\n",
    "        visualizer.close_pdf()\n",
    "        pdf_report_path = visualizer.pdf_path # Get the final path\n",
    "        if pdf_report_path and pdf_report_path.exists():\n",
    "             logger.info(f\"PDF report successfully saved to: {pdf_report_path}\")\n",
    "        elif pdf_report_path:\n",
    "             logger.warning(f\"Visualizer reported PDF path {pdf_report_path}, but file does not exist.\")\n",
    "             pdf_report_path = None # Set path to None if file doesn't exist\n",
    "        else:\n",
    "             logger.warning(\"Could not determine PDF report path from visualizer after closing.\")\n",
    "             pdf_report_path = None\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during PDF finalization: {e}\", exc_info=True)\n",
    "        pdf_report_path = None # Ensure path is None on error\n",
    "        # Attempt to close if handle seems open (defensive)\n",
    "        if hasattr(visualizer, 'pdf_pages') and visualizer.pdf_pages is not None:\n",
    "             try: visualizer.close_pdf()\n",
    "             except: pass # Ignore errors during secondary close attempt\n",
    "\n",
    "# Print final path for user\n",
    "print(f\"\\nPDF Report Path: {pdf_report_path if pdf_report_path else 'Not generated or failed'}\")\n",
    "logger.info(\"--- Step 3g: PDF Finalization Complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38780b2",
   "metadata": {},
   "source": [
    "## Step 4: Export Final Results (Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c46726",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 4: Exporting Final Results to Excel ---\")\n",
    "\n",
    "excel_path = None # Initialize path\n",
    "try:\n",
    "    # Ensure analysis results are available\n",
    "    if 'analysis_results' not in locals() or not analysis_results:\n",
    "        logger.warning(\"Analysis results missing. Skipping Excel Export.\")\n",
    "    else:\n",
    "        # --- Create Metadata ---\n",
    "        logger.info(\"Creating metadata for export...\")\n",
    "        metadata = {\n",
    "            \"Analysis Timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"Institution Analyzed\": f\"{institution_name} ({institution_short_name})\", # Use variables from Step 1\n",
    "            \"Institution Variants Used\": \", \".join(institution_names_to_match),\n",
    "            \"Input Data File\": str(data_file),\n",
    "            \"Data Update Date\": data_update_date_str,\n",
    "            \"Qualification Type Filter Applied\": \"Yes\" if filter_qual_types_flag else \"No\",\n",
    "            \"Min Market Size Threshold (for plots)\": config.get('analysis', {}).get('min_market_size_threshold', 'N/A'),\n",
    "            \"Active Qualifications Filter Threshold (for plots)\": config.get('analysis', {}).get('active_qualification_min_volume_sum', 'N/A')\n",
    "        }\n",
    "        metadata_df = pd.DataFrame(metadata.items(), columns=[\"Parameter\", \"Value\"])\n",
    "        logger.info(\"Metadata created successfully.\")\n",
    "        print(\"\\n--- Analysis Metadata ---\")\n",
    "        print(metadata_df)\n",
    "        \n",
    "        # --- Use Correct Base Output Path from Setup ---\n",
    "        # Ensure output_base_dir from setup cell is available and is a Path\n",
    "        if 'output_base_dir' not in globals() or not isinstance(output_base_dir, Path):\n",
    "            logger.error(\"Base output directory (output_base_dir) not found or not a Path object. Cannot determine Excel path.\")\n",
    "            raise NameError(\"output_base_dir is not defined or is not a Path\")\n",
    "        \n",
    "        logger.info(f\"Using base output directory for Excel export: {output_base_dir.resolve()}\")\n",
    "\n",
    "        # --- Call Exporter ---\n",
    "        # Use the wrapper function from analyze_cli which handles filename and dict prep\n",
    "        # Pass the CORRECT output_base_dir (as string) from the setup cell\n",
    "        excel_path = export_analysis_results(\n",
    "            analysis_results=analysis_results,\n",
    "            config=config,\n",
    "            institution_short_name=institution_short_name,\n",
    "            base_output_path=str(output_base_dir), # Pass the correct absolute path\n",
    "            metadata_df=metadata_df,\n",
    "            include_timestamp=ANALYSIS_PARAMS.get('include_timestamp', True)\n",
    "        )\n",
    "        \n",
    "        if excel_path:\n",
    "            logger.info(f\"Successfully exported results to Excel: {excel_path}\")\n",
    "        else:\n",
    "            logger.warning(\"Excel export function did not return a valid path.\")\n",
    "            \n",
    "        logger.info(\"--- Step 4: Excel Export Complete ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred during Excel export: {e}\", exc_info=True)\n",
    "    # excel_path remains None or its previous value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9458f0",
   "metadata": {},
   "source": [
    "## Step 5: Custom User Analysis Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"--- Step 5: Custom Analysis Area ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b51521",
   "metadata": {},
   "source": [
    "The main analysis workflow is complete.\n",
    "Key variables available for further analysis:\n",
    "- config: The loaded project configuration dictionary.\n",
    "- ANALYSIS_PARAMS: The parameters used for this specific run.\n",
    "- df_raw: The raw data loaded initially.\n",
    "- df_prepared: The cleaned and filtered data used for the main analysis.\n",
    "- analysis_results: A dictionary containing the various analysis DataFrames.\n",
    "    Keys: ['total_volumes', 'volumes_by_qualification', 'detailed_providers_market', \n",
    "           'qualification_cagr', 'overall_total_market_volume', \n",
    "           'qualification_market_yoy_growth', 'provider_counts_by_year', 'bcg_data']\n",
    "- analyzer: The MarketAnalyzer instance.\n",
    "- pdf_report_path: Path to the generated PDF report (if successful).\n",
    "- excel_path: The path to the generated Excel file (if export was successful).\n",
    "- institution_short_name: Short name of the analyzed institution.\n",
    "- institution_variants: List of variants used for matching.\n",
    "- data_update_date_str: The data update date string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434292f",
   "metadata": {},
   "source": [
    "### Additional Analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8445fb76",
   "metadata": {},
   "source": [
    "TODO: Add Section for Custom User Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb63269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # This block allows running the script directly, mimicking notebook execution\n",
    "#     logger.info(\"Running analysis script...\")\n",
    "    \n",
    "#     # Placeholder for calling the main workflow steps\n",
    "#     if 'df_prepared' in locals():\n",
    "#         logger.info(\"Data preparation step appears complete.\")\n",
    "#         if 'analysis_results' in locals() and analysis_results:\n",
    "#             logger.info(\"Market analysis step appears complete.\")\n",
    "#             # Check for PDF path instead of figures dict\n",
    "#             if 'pdf_report_path' in locals() and pdf_report_path:\n",
    "#                 logger.info(\"PDF Visualization step appears complete.\")\n",
    "#             else:\n",
    "#                 logger.warning(\"PDF Visualization step may have failed or was skipped.\")\n",
    "#         elif 'analysis_results' in locals():\n",
    "#              logger.warning(\"Market analysis step completed but produced no results.\")\n",
    "#         else:\n",
    "#             logger.warning(\"Market analysis step may have failed, 'analysis_results' not found.\")\n",
    "            \n",
    "#         if 'excel_path' in locals() and excel_path:\n",
    "#              logger.info(\"Excel export step appears complete.\")\n",
    "#         else:\n",
    "#              logger.warning(\"Excel export step may have failed or was skipped.\")\n",
    "            \n",
    "#     else:\n",
    "#         logger.warning(\"Data preparation step may have failed, 'df_prepared' not found.\")\n",
    "\n",
    "#     # Example: Accessing a config value\n",
    "#     try:\n",
    "#         data_path_from_config = config.get('paths', {}).get('data', 'Not Found')\n",
    "#         logger.info(f\"Data path from config: {data_path_from_config}\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error accessing config: {e}\")\n",
    "        \n",
    "#     logger.info(\"Analysis script finished.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be2578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vipunen-analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
